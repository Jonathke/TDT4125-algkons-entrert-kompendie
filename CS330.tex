%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{
    \documentclass[twoside,11pt]{article}
    %%%%% PACKAGES %%%%%%
    \usepackage{pgm2016}
    \usepackage{amsmath}
    \usepackage{algorithm}
    \usepackage[noend]{algpseudocode}
    \usepackage{subcaption}
    \usepackage[english]{babel}	
    \usepackage{paralist}	
    \usepackage[lowtilde]{url}
    \usepackage{fixltx2e}
    \usepackage{listings}
    \usepackage{color}
    \usepackage{hyperref}
    \usepackage{amssymb}
    
    \usepackage{auto-pst-pdf}
    \usepackage{pst-all}
    \usepackage{pstricks-add}
    
    %%%%% MACROS %%%%%%
    \algrenewcommand\Return{\State \algorithmicreturn{} }
    \algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}
    \renewcommand{\thesubfigure}{\roman{subfigure}}
    \definecolor{codegreen}{rgb}{0,0.6,0}
    \definecolor{codegray}{rgb}{0.5,0.5,0.5}
    \definecolor{codepurple}{rgb}{0.58,0,0.82}
    \definecolor{backcolour}{rgb}{0.95,0.95,0.92}
    \lstdefinestyle{mystyle}{
       backgroundcolor=\color{backcolour},  
       commentstyle=\color{codegreen},
       keywordstyle=\color{magenta},
       numberstyle=\tiny\color{codegray},
       stringstyle=\color{codepurple},
       basicstyle=\footnotesize,
       breakatwhitespace=false,        
       breaklines=true,                
       captionpos=b,                    
       keepspaces=true,                
       numbers=left,                    
       numbersep=5pt,                  
       showspaces=false,                
       showstringspaces=false,
       showtabs=false,                  
       tabsize=2
    }
    \lstset{style=mystyle}



    \firstpageno{1}
    
    \begin{document}

\tableofcontents
\newpage
\section{Matching}
Matching er et ganske greit problem, men som går igjen mye på starten og i eksempler av forskjellige konsepter, så greit å ha god kontroll på det. Generelt så er det et av de få graf-optimaliseringsprobleme som kan løses i polynomisk tid. Problemet er formulert som følger: Gitt en graf $G = (V,E)$, finn en $M \subseteq E$ slik at grafen $G' = (V,M)$ ikke inneholder noen noder av grad $> 1$. Det finnes fire forskjellige tilfeller, som alle kan sees på som ganske forskjellige problemer. De kommer av hvorvidt grafen er bipartitt og/eller vektet. 

Trolig det viktigste konseptet for å oppnå optimale løsninger på matching-problemer er forøkende stier. Først må vi ha litt terminologi: En maksimal løsning er en ikke-utvidbare løsning. Et maksimum er den optimale løsningen. Åpenbart er et maksimum også en maksimal løsning. En forøkene sti er en sti der kantene veksler mellom å være kanter som er med i en matching, og ikke med i en matching. Dersom en slik sti starter å slutter i noder som ikke er med i matchingen, så er den forøkende. For å se dette, bare "flip" alle kantene i stien (fjern de som er med i matchingen, legg til de som ikke er det). Dette vil naturligvis øke matchingen med 1 kant (husk at stien startet og sluttet i noder som ikke var med i matchingen).

Det kanskje første teoremet i algkons er da at en løsning er et globalt maksimum hvis og bare hvis det ikke finnes noen forøkende stier. Ganske åpenbart den ene veien, men litt mer overraskende at ingen forøkende stier er et tilstrekkelig krav for å oppnå globalt maksimum. For å se dette, si at vi har en graf med en matching $M$ uten noen forøkende stier. I tillegg antar vi at det finnes en $M'$ slik at $|M'| > |M|$. Dersom vi da ser på grafen der kantene er i $M \oplus M'$, kan vi si et par ting om alle stiene. Her må naturlig nok alle stier alternere mellom $M \text{ og } M'$. Dermed finnes det ingen odde sykler. Videre, siden  $|M'| > |M|$ må det finnes minst en vei som både starter og slutter i $M'$. Dette vil da være en alternerende sti i $G = (V,M)$, så vi har oppnådd en kontradiksjon. Dermed kan vi naturlig nok generelt basere uvektet matching algoritmer på slike forøkende stier (detaljer i boka).

\subsection{Bipartitt Matching}
Når grafen er bipartitt ($G = (U \cap V, E)$) blir matching problemet noe "enklere". Men, i flere virkelighetstilfeller modellerer vi faktisk bipartitt matching, og ikke bare matching (typiske "assignement" problemer). Naturlig nok kan vi her også basere løsninger på forøkende stier, og det gir også opphav til et nytt teorem. Først, for å se hvordan man lager en slik forøkende sti i en bipartitt graf, start me en hvilken som helst umatchet node i $U$. Stien vil naturligvis alternere mellom $U$ og $V$, og må også ende i $V$ for å være forøkende. Dette gir oss da \textbf{Hall's Marriage Theorem}, som forteller oss at en bipartitt graf $G = (U \cap V, E)$ har en perfekt matching hvis og bare hvis $|S| \leq |N(S)|, \forall S \subseteq U$, der en perfekt matching er en mathing der alle noder har grad nøyaktig lik $1$, og $N(S) = \{u \in V : (u,v) \in E, v \in S\}$ (eller sagt med ord, "naboene" til nodene $v \in S$). Dette er åpenbart nødvendig (ellers ville det ikke eksistert nok noder i $N(S)$ til å matche alle nodene i $S \subseteq U$), men det er også tilstrekkelig. For å se dette, la oss si at vi har en udekt node $u \in U$ (ikke perfekt altså), og at det ikke eksisterer noen forøkende stier (løsningen er med andre ord globalt maksimum). For at dette skal stemme må da $S \setminus \{u\}$ være matchet med $T$, og vi har $|T| = |N(S)| = |S| - 1 \iff |S| > |N(S)|$.

For uvektet bipartitt matching er altså disse forøkende stiene den naturlige måten å optimere problemet på. Videre finnes det et par andre smarte ting å gjøre, f.eks. Hopcroft-Karp som istedenfor å finne en og en forøkende sti finner den alle disjunkte forøkende stier. Dette forbedrer kjøretiden fra $\mathcal{O}(nm)$ til $\mathcal{O}(\sqrt{n}m)$. Andre varianter er å legge til en kilde og et sluk på hver "side" av den bipartitte grafen, og løse med maks flyt.

\subsection{Vektet Bipartitt Matching}
Dette løses med den ungarske metoden (se kapittel \ref{ungarn}). Men det gis en liten teaser til tema om grådighet i forelesning 1, så jeg legger det ved her og. En helt naiv grådig løsning, alltid velge den tyngste/letteste (avhengig av om det er et maksimerings eller minimeringsproblem) kanten som er lovlig, gir en $2$-approksimasjon, ettersom optimeringsproblemet bipartitt matching er snittet av to transversalmatroider. Les videre for å forstå hva den setningen betyr... (Det viser seg faktisk at en grådig løsning gir en $2$-approksimasjon i det generelle, ikke-bipartitte tilfelle også).
\subsection{Vektet (Ikke-Bipartitt) Matching}
Løses gjerne med Lineær Programmering. Se neste seksjon.


\newpage
\section{Lineær Programmering}
Dette er kanskje det viktigste konseptet i faget, og grunnlaget for omtrent hele første halvdel av pensum. Lineære programmer er en veldig generell måte å modellere/sette opp optimaliseringsproblemer over $\mathbb{Q}^n$, der løsningen består av å optimalisere en lineær målfunksjon, samtidig som vi holder oss innen lineære restriksjoner. Lineær program har en veldig visuel løsningsform, der restriksjonene tilsammen lager en (konveks) polytop av gyldige løsninger, og målfunksjonen bestemmer retningen hvor den optimale løsningen ligger. Dessverre så kan også restriksjonene være umulige, eller ubegrenset. Men dersom det ikke gjelder, kan vi generelt finne den optimale løsningen i polynomisk tid!

\subsection{Kanonisk Form}

Generelt ser den kanoniske formen på lineær program slik ut:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \boldsymbol{c}^T &\boldsymbol{x}&\\
\text{subject to:}& A&\boldsymbol{x} \leq \boldsymbol{b}\\
&&x_i \geq 0&\forall x_i \in \boldsymbol{x}
\end{array}
\end{equation*}

Her er $A$ en matrise, mens $w,x,b$ er vektorer. Grunnen til at en slik "kanonisk form" er mulig er ganske enkelt fordi alle lineærprogram kan omformuleres til denne formen. Det gjøres slik: $min(cx) = max(-cx), x \geq b \iff -x \leq b, x = b \iff x \leq b \wedge x \geq b$. Merk at både gjennom boka og kompendiet så brukes $min,max$ om hverandre (begge er definert som kanonisk form). Dette fordi at enkelte problem er naturlig å tenke på som maksimeringsproblemer, mens andre er mer naturlig å tenke på som minimeringsproblemer. Uansett, det er ønskelig å få alle ulikhetene samme vei for restriksjonene, så dette gjøres stort sett også.

\subsection{Heltallsprogram}
Veldig mange praktiske (og teoretiske) problemer har en form for heltallighet som krav. Hvis $\boldsymbol{x}$ representerer antal av forskjellige komponenter i en optimal løsning, er det naturlig å tenke seg at alle $x_i \in \boldsymbol{x}$ må være heltallige, avhengig av optimaliseringsproblemet (hvis $\boldsymbol{x}$ representerer mennesker fra forskjellige grupper f.eks. så er det jo uheldig å få en fraksjonell løsning). Dermed kan dette være en naturlig restriksjon på lineærprogrammet vårt. Da ender vi opp med det som kalles et heltallsprogram. Fra nå av bruker vi IP og LP (IP = Integer Program, LP = Linear Program). En annen variant av IP kan modelers som at $x_i \in \{0,1\}$. Dette gir en veldig naturlig måte å formulere "decision"-type problemer, der $0,1$ representerer valg (typisk ta med i løsningen eller ikke). Som vi vet er slike "decision"-problemer veldig ofte NP-harde, noe som betyr at det å løse et IP er generelt NP-hardt også.

Likevel så er det flere ting vi kan gjøre for å jobbe rundt dette, og heltallsprogram er den absolutt sentrale delen av pensum, og helt håpløst er det ikke. For det første har enkelte lineær-program har alltid heltallige løsninger (selvom det ikke har heltallskrav), og for det andre kan ofte løsningen på slakkingen av IPet fortelle oss noe om løsningen (hvis vi klarer å argumentere for en øvre grense på forskjellen mellom løsningen på IPet og det tilhørende LPet). En slakking av et heltallsprogram er når vi fjerner heltallskravet og ender opp med et tilhørende lineærprogram.

\subsection{Dualitet}
Også et utrolig viktig konsept som går igjen i første halvdel av pensum (mantraen er generelt lineærprogram++ går igjen i første halvdel...). For ethvert lineærprogram, kan vi enkelt konstruere det såkalte "duale" lineærprogrammet. Dette omtales som DUALEN, mens det originale lineærprogrammet da omtales som PRIMALEN. Vi lager dualen ved å sette en variabel for hver restriksjon i primalen, og en restriksjon i dualen for hver variabel i primalen. En intuitiv forståelse av dualen er litt vanskelig å skrive ned, men det kommer når det jobbes med. Det som er målet er å finne en øvre grense for den optimale løsningen for primalen, og det viser seg at denne øvre grensen også er et lineær program. Dualen til den kanoniske formen nevnt tidligere ser slik ut, og forsøker å skvise en øvre grense så nært som mulig:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Mimimize: }  & \boldsymbol{b}^T &\boldsymbol{y}&\\
\text{subject to:}& A^T&\boldsymbol{y} \geq \boldsymbol{c}\\
&&y_i \geq 0&\forall y_i \in \boldsymbol{y}
\end{array}
\end{equation*}

Hadde vi tatt dualen av dette lineær-programmet igjen, hadde vi endt opp med den kanoniske formen vi starta med. Dette er generelt også alltid sant (Dualen til DUALEN er igjen PRIMALEN). 

For gyldige løsninger på primalen og dualen kan vi generelt skrive følgnde ulikheter

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Primal: }  & &A\boldsymbol{x} \leq \boldsymbol{b}&\\
\text{Dual:}& \boldsymbol{c} \leq \boldsymbol{y}&A\\
x,y \geq 0 &\boldsymbol{c}\boldsymbol{x} \leq \boldsymbol{y}&A\boldsymbol{x} \leq \boldsymbol{y}\boldsymbol{b}&\\
\text{Ved optimum (!):}&\boldsymbol{c}\boldsymbol{x} = \boldsymbol{y}&A\boldsymbol{x} = \boldsymbol{y}\boldsymbol{b}&\\
\end{array}
\end{equation*}
Den nest nederste kalles ulikheten kalles svak dualitet, mens den nederste likheten kalles sterk dualitet. Beviset for sterk dualitet kan sees i boken.

Dette gjelder naturlig nok kun når både primalen og dualen er endelig optimale (det holder å vise at en av dem er det, for da må også den andre være det). Generelt finnes det 4 tilfeller, som jeg lister raskt opp nå; 1) PRIM endelig optimal, DUAL endelig optimal, 2) PRIM ubegrenset, DUAL umulig, 3) PRIM umulig, DUAL ubegrenset, 4) begge umulig. 

\subsection{Komplementær slakkhet}
Et viktig teorem, basis for primal-dual metoden som det står skrevet om litt lenger nede. Komplementær slakkhet er følgende "betingelse" for et PRIMAL/DUAL par:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
y_i = 0 \Longleftarrow (Ax)_i \neq b_i & \forall i\\
x_j = 0 \Longleftarrow (yA)_j \neq c_j & \forall j\\
\end{array}
\end{equation*}

Det er lett å se at komplementær slakkhet impliserer likhet mellom målfunksjonene (at det er en tilstrekkelig betingelse for likhet). Men at kanskje noe mer overraskende resultat er at komplimentær slakkhet også er nødvendig for å oppnå likhet! Dvs. likhet mellom målfunksjonene impliserer komplimentær slakkhet. Her er et pent bevis:

Se for oss at vi har likhet mellom målfunksjonene, altså $\boldsymbol{c}\boldsymbol{x} = \boldsymbol{y}A\boldsymbol{x} = \boldsymbol{y}\boldsymbol{b}$. Den siste halvdelen kan vi skrive om som $\boldsymbol{y}(\boldsymbol{b}-A\boldsymbol{x}) = 0$, og vi vet at $\boldsymbol{b}-A\boldsymbol{x} \geq 0$, siden restriksjonen var at $A\boldsymbol{x} \leq \boldsymbol{b}$. Dette gir oss følgende:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\boldsymbol{y}(A\boldsymbol{x}-\boldsymbol{b}) = &\sum\limits_{i=1}^{n}y_i(Ax-b)_i = 0\\
& (Ax-b)_i \geq 0 & \forall i \in \{1,..,m\}\\
& y_i \geq 0 & \forall i \in \{1,..,m\}
\end{array}
\end{equation*}

og det er helt tydelig at vi må ha komplimentær slakkhet (dette viser strengt tatt bare komplimentær slakkhet den ene veien, men et helt symmetrisk argument gir beviset andre veien og).

\newpage

\section{Den ungarske metoden} \label{ungarn}

\newpage

\section{Approksimasjon}
\label{sec:approx}


Approksimasjonsalgoritmer er algoritmer for å løse optimaliseringsproblemer. Konseptet som introduseres er litt løst basert på følgende utsagn: For å løse optimaliseringsproblemer trenger man 1) Optimal løsning, 2) I rask tid, 3) for alle problemer. Velg 2. Approksimeringsalgoritmer slacker på kravet om optimal løsning, ved å heller gi et svar som er så nært som mulig optimalt, men ikke nødvendigvis optimalt.

For å få noe av verdi må vi kunne si noe om hvor nære approksimasjonen er, eller approksimasjonsgraden. La APX og OPT være hhv. approksimasjonen og den optimale løsningen. Da sier vi at algoritmen er en $\alpha$-approksimasjon dersom.

\begin{align} \label{eq_01}
	&APX \geq \frac{1}{\alpha}OPT \text{ For maksimeringsproblemer}\\ 		
	&\frac{1}{\alpha}APX \leq OPT \text{ For minimeringsproblemer}
\end{align}

Et viktig konsept som går igjen er PTAS vs FPTAS. Definisjonen på en PTAS er en familie med approksimeringsalgoritmer $\{A_\epsilon\}_{\epsilon>0}$, der alle $A \in \{A_\epsilon\}_{\epsilon>0}$ er ($1 + \epsilon$)-approksimasjoner, med polynomisk kjøretid i n. En FPTAS er en PTAS der kjøretiden også er polynomisk i $\frac{1}{\epsilon}$. For å illustrere forskjellen oppfyller $O(n^{\frac{1}{\epsilon}})$ kun kravet til PTAS, mens $O(n^c + (\frac{1}{\epsilon})^d)$ oppfyller kravet til FPTAS også.

\subsection{Maximum Independent set/Største Uavhengige Set}
For en graf $G = (V,E)$, kalles en $I\subseteq V$ independent/Uavhengig dersom ingen to noder i $I$ er koblet sammen.

Å finne det største kan skrives som et lineærprogram på følgende måte

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\nolimits_{v\in V} &x_v &\\
\text{subject to:}&x_u + &x_c \geq 1,  &\text{ for alle kanter $(u,v)\in E$}\\
                 &                                                &x_v \in \{0,1\}, &\text{ for alle noder $v\in V$}
\end{array}
\end{equation*}

Dette er hovedsaklig tatt med som et eksempel på problemer som ikke kan approksimeres (Ikke 100\% sikker på dette, men tror det er et teorem om at det ikke finnes noen PTAS for største uavhengige sett, med mindre $P = NP$). Det samme gjelder for Maksimum Clique problemet (Hvordan disse problemene er relatert overlater jeg til leseren).

\subsection{Set cover/Mengdedekke}
Mengdedekke er et veldig generelt problem, formulert som følgende: For et grunsett $E = \{e_1,e_1,...,e_m\}$ har vi flere subset $S_1, S_2,...,S_n$ der $S_i\subseteq E$, samt en tilknyttet vekt $w_i$ for hver $S_i$. Målet er å finne den sammensetningen av subset som dekker hele grunsettet med lavest vekt. Dersom $w_i=1, \forall i \in \{1,..,n\}$ har vi et \emph{uvektet} mengdedekke problem. Formulert som et lineær program har vi altså følgende:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\limits_{i = 1}^{n} &w_ix_i &\\
\text{subject to:}&\displaystyle\sum\nolimits_{j:e \in S_j} &x_j \geq 1,  &\text{ for alle $e\in E$}\\
                 &                                                &x_i \in \{0,1\}, & i \in \{1,...,n\}
\end{array}
\end{equation*}

F.eks. er da et nodedekke et spesialtilfelle av mengdedekke. For å redusere fra en generell instans av et nodedekke til mengdedekke, la grunsettet være alle kanter. For hver node, lag et subset med alle tilkoblede kanter (slik at $n=|V|$), og la $w_i = v_i$. 

I øving 2 er det et annet eksempel der mengdedekke reduseres til \emph{uncapacitated facility problem} og motsatt.

\subsubsection{Approksimering ved avrunding}\label{{sec:approksavrunding}}
Den første taktikken for å approksimere mengdedekke er å approksimere ved avrunding. Det fungerer ved først å løse slakkingen til lineærprogrammet, finne $f = \text{max}(|\{j:e\in S_j\}|)$, og deretter runde alle $x_i \geq \frac{1}{f}$ opp til $1$, og sette resten til $0$. Dette gir en $f$-approksimering. En intuitiv tolkning av f her er maksimalt antal subset et element opptrer i. Det er tydelig at dette fremdeles er en gyldig løsning til lineærprogrammet, ettersom $\sum\nolimits_{j:e \in S_j} x_j \geq 1$ i den optimale løsningen (siden summen ikke har mer enn $f$ ledd må minst et av dem være mer enn $\frac{1}{f}$).

Dette er grunnen til at relaksering med avrunding løser nodedekke som en $2$-approksimasjon. Hver kant (hvert element) er intil nøyaktig to noder (del av to subset). Dermed har vi $f=2$.


\subsubsection{Approksimering ved dualbasert avrunding}
Dualen til (LP relakseringen av) lineærprogrammet til mengdedekket ser ut som følger:

\begin{equation*}\label{DUALsetCover}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\limits_{i = 1}^{n} &y_i &\\
\text{subject to:}&\displaystyle\sum\limits_{i : e_i \in S_j} &y_i \leq w_j,  &\text{ for alle $j\in \{1,...,n\}$}\\
                 &                                                &y_i \geq 0 &\text{ for alle $i\in \{1,...,n\}$}
\end{array}
\end{equation*}

Husk: I primalen hadde vi 1 restriksjon pr. element. Dvs. at hver $y_i$ tilsvarer en restriksjon, eller et element. I tillegg har vi i dualen en restriksjon pr. delmengde i primalen. Den intuitive moten å forstå dualen på kan være at hvert element har en pris. Denne prisen til et sett (summen av prisen til alle elemntene i settet) kan ikke overskride vekten av settet. Målet er da å maksimere prisene $y_i$.

Ideen er nå å bruke noe av konseptet bak komplimentær slakkhet, samt den optimale løsningen til dualen for å konstruere en god approksimasjon til primalen. Vi gjør det på følgende måte: Løser dualen og får $\boldsymbol{y^*_i}$. Deretter løser vi primalen ved å velge alle de $S_j$ som er slik at $\sum\nolimits_{i:e_i \in S_j}\boldsymbol{y^*_i} = w_j$, dvs der dualrestriksjonen er stram. Dette fungerer ettersom hvis det skulle ha fantes et element som ikke var dekket, ville det betydd restriksjonen ikke var stram for noen av mengdene som elementet var del av. Dermed er løsningen heller ikke optimal siden vi kunne uten problem økt prisen på elementet.

Hva får vi fra dette? Jo, det blir en ny $f$-approksimasjon, f samme som i forrige seksjon. Det er hovedsaklig to ting som er sentrale for å se dette. For det første, siden $DUAL \leq OPT$, har vi at $\sum\limits_{i=1}^{n}y_i \leq OPT$. Videre er $S_j$ kun med i løsningen dersom $\sum\nolimits_{i:e_i\in S_j}y_i = w_j$. Resten klarer du å sette sammen selv (eller se i boka...).

\subsubsection{Approksimering ved grådighet}
Den siste metoden for approksimering av mengdedekke er presentert som en grådig løsning (og her den suverent beste!). Dette er en $H_n$-approksimering ($H_n = \sum\limits_{k=1}^{n}\frac{1}{k}$, AKA. summen av de n første leddene i den harmoniske rekken), der $n=|V|$. Den er ganske enkel å forklare. Approksimasjonsgraden er litt mer å forklare. Algoritmen fungerer slik: Ved hvert steg, regn ut det settet som gir mest lavest stykkpris på udekte elementer, og legg til dette settet i løsningen. Itererer til alle elemter er dekket (maks n ganger). Litt mer formelt, la $I$ være den nåverende løsningen, og la $\hat{S_i} = S_i\setminus\{e\in I\}$. Ved hver iterasjon gjør $I = I \cup S_j : j = min_i(\frac{|S_i|}{w_i}), \forall i$. 

For bevis av approksimasjonsgraden her, se boka, side 108.
\newpage

\section{Primal-Dual Metoden}
Dette er den generelle ungarske metode/dualbasert avrunding i mengdedekke metoden, og gir ofte opphav til gode approksimeringsalgoritmer. I tillegg er approksimeringsgraden ofte lett å analysere.

Metoden er veldig generell, og går ut på å starte med en gyldig dual-løsning (ikke nødvendigvis opptimal), og så gradvis forbedre dualen samtidig som man bygger løsningen til primalen (ofte basert på komplimentær slakkhet). Analysen av løsningen går da ofte ut på at vi har komplimentær slakkhet en vei, mens en approksimert komplimentær slakkhet den andre veien. Det enkleste er nesten bare å se på eksempler, så vi kjører på.

\subsection{Mengdedekke revisited}
Se over for en beskrivelse av mengdedekke. Primal-dual metoden for mengdedekke gir opphav for løsningen approksimering ved dualbasert avrunding, kun med en liten endring. I steden for å starte med å konstruere en optimal løsning til dualen $\boldsymbol{y^*_i}$, så starter vi heller med en gyldig løsning, f.eks $y_i = 0, \forall i$. Deretter gjør vi som i analysen av gyldigheten, og ser på et vilkårlig udekt element $e_i$. Siden elementet er udekt, betyr det at $\sum\nolimits_{k:e_k\in S_j}y_k < w_j,\forall j:e_i \in S_j$, og dermed kan vi øke $y_i$ med en $\epsilon > 0$, slik at minst én dualrestriksjon blir stram. Da legger vi til $S_j$ i løsningen for alle $j : \sum\nolimits_{k:e_k\in S_j}y_k = w_j$, altså der dualrestriksjonen er stram. Slik kan vi bygge løsningen vår med maks $n = \text{antall elementer}$ iterasjoner.

Her ser vi at komplimentær slakkhet åpenbart er oppfylt den ene veien, siden vi valgte å sette $x_j = 1 \iff \sum\nolimits_{k:e_k\in S_j}y_k = w_j$. For alle andre har vi $x_k = 0$. Andre veien derimot har vi kun approksimert komplimentær slakkhet. Dette fordi at for en $y_i > 0$ så vil $\sum\nolimits_{j : y_i \in S_j}x_j \leq f$, der $f$ er som tidligere (maks antall ganger et element opptrer i forskjellige set). Dermed er det også lett og se at dette blir en $f$-approksimasjon.

\subsection{Feedback Vertex Set/Sykelkritiske Noder}
Dette problemet kan tolkes på flere måter. Personlig er den mest intuitive måten som følger: Vi har en urettet graf $G = (V,E)$, med tilhørene vekter $w_i$ til nodene. Vi ønsker å fjerne noder billigst mulig slik at $G$ blir asyklisk. Som et IP (der $\boldsymbol{C}$ betegner settet av sykler i $G$):

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\nolimits_{i \in V} w_i&x_i &\\
\text{subject to:}&\displaystyle\sum\nolimits_{i \in C} &x_i \geq 1,  &\forall C \in \boldsymbol{C}\\
                 &                                                &x_i \in \{0,1\} &\forall i \in V
\end{array}
\end{equation*}

Her er det viktig å se at $\boldsymbol{C}$ vokser eksponensielt med antall noder, så det blir mange restrikssjoner etterhvert. Tar vi relakseringen av IP formuleringen vår, og ser på dualen har vi

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\limits_{C \in \boldsymbol{C}} &y_c &\\
\text{subject to:}&\displaystyle\sum\limits_{C \in \boldsymbol{C}:i \in C} &y_c \leq w_i,  &\forall i \in V\\
                 &                                                &y_c \geq 0 &\forall C \in \boldsymbol{C}
\end{array}
\end{equation*}

Nå har vi naturlig nok eksponensielt mange dualvariabler (siden vi hadde eksponensielt mange primalrestriksjoner), men er ikke et stort problem, ettersom vi kun trenger å øke et polynisk antall av dem). Den naturlige måten å løse dette på er å forsøke nøyaktig samme strategi som i forrige seksjon, starte med gyldig dual, også øke den "nærmeste" $y_c$ slik at minst en restriksjon blir stram. Samme analyse som tidligere oppnår vi da følgende approksimasjonsgrad (her betegner $S$ løsningssettet):

\begin{align} \label{eq_01}
	&\sum\limits_{i \in S}w_i = \sum\limits_{i \in S}\sum\limits_{C:i\in C}y_c = \sum\limits_{C \in \boldsymbol{C}}|S\cap C|y_c
\end{align}

Her ser vi altså at vi får en approksimasjonsgrad på $max_{C}(|S\cap C|)$ (maks antall sykler en node i løsningen er med i). Dessverre vil samme naive "bare øk hvilken som helst dualvariabel" strategi som tidligere føre til at dette tallet kan bli veldig høyt. Men ved litt smartere valg av variabler vil vi kunne oppnå en ganske god approksimasjonsgrad.

Løsningen er å i et hvert steg kun øke dualvariabler som tilhører sykler med mindre enn $2\lceil \log_2n\rceil$ noder av grad $3$ eller mer. Det er en del analyse bak dette, men her kommer en rask oppsummering: Vi fjerner alltid noder av grad $< 2$ siden disse umulig kan være del av en sykel. Stier av noder av grad $2$ vil utslettes ved å velge en vilkårlig node i stien (dette gjøres iteratit siden naboer da vil ha grad $< 2$). Dermed kan disse stiene slåes sammen til en kant. Deretter har vi et lemma som sier at i en graf uten noder av grad $1$ finnes det alltid en sykel med maks $2\lceil \log_2n\rceil$ noder av grad $3$ eller mer. Dette sees ved å se for seg et binærsøk gjennom grafen, der vi slår sammen stier av noder med grad $2$. Her vil entall noder minst doble seg per nivå gjennom treet, og vi kan ha maksimal dybde på $\lceil \log_2n\rceil$. Dermed (om det finnes minst en sykel), vil det finnes minst en sykel som også har lengde mindre enn $2\lceil \log_2n\rceil$ (se for deg at den knyttes sammen helt i bånn av treet).

Denne strategien gir oss en $4\lceil \log_2n\rceil$-approksimasjon. For å se dette, se først at størrelsen på sykler som "legges til" (dualvariablen økes) i hvert trinn av algoritmen ikke overskrider $2\lceil \log_2n\rceil$, hvis vi kun teller noder av grad $> 2$. Teller vi bare disse nodene, så har vi $|S\cap C| \leq 2\lceil \log_2n\rceil$. Men, det kan også være stier med noder av grad $2$ i mellom hver av nodene. Uansett, fra disse stiene er maksimalt en node med (se forrige avsnitt). Dermed kan har vi $|S\cap C| \leq 4\lceil \log_2n\rceil$, og som tidligere argumentert for gir dette en øvre grense på approksimasjonsgraden.

Dette er en viktig lekse i primal-dual algoritmen. Ofte ønsker vi å øke den "minste" (på et eller annet vis) dual-variabelen.

\subsection{Korteste vei}
Mer av det samme som i forrige seksjon, det vanskeligste her er oppsettet. Hopper over det formelle, kan evt. se i boka. Her har vi primalvariabler som kanter som skal skrus av/på, mens dualvariablene er s-t kutt, dvs. $S = \{v \in V:s \in S, v \notin S\}$. Er egentlig ingen ny lekse her, men igjen øker vi alltid den $y_s$ som tilhører den "minste" $S \in \boldsymbol{S}$. Minste her betyr det subsettet av noder vi kan nå med løsningen vi har bygget til ved den nåværende iterasjonen, og øker til en av dualrestriksjonene blir stramme. Da tar vi med den kanten i løsningen. Tilslutt, når vi når $t$ så stripper vi vekk alt som ikke er del av s-t veien og returnerer løsningen $P$.

Resultatet her blir da en velkjent algoritme, Dijkstra's algoritme, som finner den optimale løsningen. Dette sees ved standard primal-dual analyse, som gir oss at dette er en $max(|P\cap \delta (S)|)$-approksimasjon, der $\delta (S) = \{e = (u,v): u \in S, v \notin S \}$. Deretter kan man se at $|P\cap \delta (S)| = 1, \forall y_c \neq 0$, siden ellers ville vi på et punkt lagt til en sykel i løsningen vår. Dermed får vi en optimal løsning (det er jo velkjent at Dijkstra's returnerer den optimale løsningen, så dette burde vel ikke være noen bombe).

\subsection{Minimum knapsack problem}
Denne seksjoner viser et annet fenomen som kan være et hinder for approksimeringsalgoritmer. Problemet er som et slags motsatt knapsack-problem, der man ønsker å bære lavest mulig vekt, men der verdien av tingene er høyere enn en satt terskel. En naturlig formulering av IP-et gir:

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\limits_{i \in I} s_i&x_i &\\
\text{subject to:}&\displaystyle\sum\limits_{i \in I} v_i&x_i \geq D\\
                 &                                                &x_i \in \{0,1\} &\forall i \in I
\end{array}
\end{equation*}

Her er $s_i$ og $v_i$ hhv. vekten og verdien til objektet $i \in I$. Dette oppsettet gir opphav til et arbitrært dårlig "Integrality gap", eller heltallsavvik. Heltallsavvik er betegnelsen på den øvre grensen for et tall $\alpha$ slik at $\frac{OPT}{PRIM} \geq \alpha$ (for et minimeringsproblem). I primal-dual analysen er det helt klart nødvendig at dette tallet ikke er for høyt, ettersom det gjør at løsningen på relakseringen forteller oss lite om den faktiske løsningen. 

I vårt nåværende oppsett kan man gi problemet $I = \{1,2\}$, hvor $v_1 = D-1, v_2 = D, s_1 = 0, s_2 = 1$. Her er det lett og se at heltallsløsningen er å kun ta med objekt $2$, og gir oss summen $1$, mens LP-relakseringen vil ta med objekt $1$, og fylle opp siste lille biten med en fraksjon av objekt $2$. Dette gir summen $\frac{1}{D}$, og gir oss heltallsavvik $\frac{OPT}{PRIM} = D$.

Det er en annen lur måte å sette opp problemet på som unngår det skrekkelige heltallsavviket ved å bytte ut verdi-begrepet med "avstand" fra å fylle opp til kravet D, og gir opphav til en $2$-approksimering. Se boka (s. 127)/forelesning 5 for detaljer.

\newpage
\section{Grådige algoritmer og lokale søk}
Grådige algoritmer er noe som bør være kjent (kanskje til og med som noe av det enklere) fra tidligere fag. Lokalt søk har noen likheter med grådige algoritmer. Begge har den store fordelen av at de ofte er svært enkle å implementere, og gir også ofte grunnlag for teoretisk analyse av approksimeringer. 

Grådige algoritmer, formulert med språket som vi er blitt vant med nå, er algoritmer som starter med en ugyldig primal, og bygger løsningen gradvis. Alle valg som tas er bindende. Lokale søk derimot starter med en (gjerne arbitrær) gyldig primal og forsøker å opptimalisere den, mens den holder seg gyldig. Her er ingen valg bindende. Vi knekker går i denne seksjonen raskt gjennom tre greie eksempler og analyserer approksimasjonsgraden.

\subsection{Sekvensering (parallelle maskinjobber)}
Dette er et greit problem, hvor vi har $m$ maskiner som skal fullføre $n$ jobber $j = 1...n$, der jobb $j$ har prosesseringstid $p_j$. En maskin kan kun holde på med en jobb om gangen, og må fullføre den når den er startet på. Målet er å minimere tiden det tar å fullføre alle jobbene, omtalt som $C_max$.

Et enkelt lokalt søk er som følger: Start med et arbitrært oppsett av jobber. Deretter flytt den jobben som er ferdig sist til den maskinen som i øyeblikket er ferdig tidligst, slik at $C_{max}$ synker. Fortsett å gjøre dette til det ikke lenger går (at jobben blir ikke blir flyttet), og output oppsettet. 

For analysen av approksimasjonsgraden her trenger vi to nedre grenser for $OPT = C^*_{max}$. De to åpenbare er at $C^*_{max} \leq max_j p_j$, og at $C^*_{max} \leq \sum\nolimits_j \frac{p_j}{m}$. Vi deler så inn i to intervaller for algoritmens output, før og etter siste jobb $p_j$ begynner (vi kaller det tidspunktet $s_j$. Ved tidspunktet $s_j$ har alle m maskinene jobbet konstant, og jobben j har ikke blitt startet på enda, altså har vi at $s_j \leq \sum\nolimits_{i\neq j} \frac{p_i}{m}$. Videre er $p_j \leq max_i p_i$. Dermed er det ikke vanskelig å se at $C_{max} = s_j + p_j = \sum\nolimits_{i\neq j} \frac{p_i}{m} + p_j = (1-\frac{1}{m})p_j + \sum\nolimits_i \frac{p_i}{m} \leq (2-\frac{1}{m})C^*_{max}$. Altså er vårt lokale søk en $(2-\frac{1}{m})$-approksimasjon.

For den grådige varianten av problemet, bare plasser ut jobber så snart en maskin har blitt ledig. Analysen her er nå mega-enkel, siden vi ser at når den grådige algoritmen er ferdig, vil den ha samme output-form som det lokale søke (altså hvis vi hadde forsøkt å kjøre det lokale søke på outputen til den grådige algoritmen, ville den bare ha outputta med en gang). Dermed er også dette en $(2-\frac{1}{m})$-approksimasjon. Dette kan videre forbedres til en $\frac{4}{3}$-approksimasjon ved å sortere lista av jobber for de mates til den grådige algoritmen. Detaljer finnes i boka/forelesning 6.

\subsection{Klynge-analyse}
Her er oppgaven rimelig grei, målet er å finne $k$ senter blandt $n$ punkter. Slike senter er da noder som er "nærmest de andre". Litt mer formelt er målet å finne de punktene som gjør at radiusen til de sirklene som har punktene i midten og omfavner alle noder i grafen blir minst mulig. Den sentrale antagelsen i problemet er at vi befinner oss i såkalte metriske rom, der grafen er komplett, og kantene har vekter som kan sees på som "distanse" mellom nodene, altså at $d_{ij} \geq 0$ og at $d_{ik} + d_{kj} \geq d_{ij}$ (The triangle-inequality) holder for alle noder.

Her konstruerer vi enkelt en grådig algoritme som starter med å velge en arbitrær node å legger til i løsningen $S$. Så, for hver iterasjon tar den den noden som ligger lengst unna alle andre noder i $S$ og legger til i $S$. Dette fortsetter til $|S| = k$. Dette gir enkelt og greit en $2$-approksimasjon. For å se dette, anta først at alle noder i $S$ befinner seg i hver sin sirkel, med hensyn til $OPT = S^*$. Da ser vi med en gang at dette er en $2$-approksimasjon (en radius på $2r^*$ dekker hele sirkelen uansett hvor noden er plassert). Dersom $S$ inneholder to noder fra samme sirkel, betyr det at da det var den lengste avstanden i systemet, var maksimal avstand $\leq 2r^*$.

Det mest interessante her er at selvom det var såpass enkelt å oppnå en $2$-approksimasjon, ville det å oppnå så mye som en mikroskopisk smule bedre implisere at $P=NP$ . Grunnen til dette er at vi kunne brukte en $(2-\epsilon)$-approksimasjon for dette problemet til å bestemme om en graf har en dominerende mengde av $k$ noder (der $N(S) = V, S \subseteq V, |S| = k$) , et problem som er NP komplett. For å se dette, set $d_{ij} = 1$ dersom $(i,j)\in E$, ellers set $d_{ij} = 2$. Da holder fortsatt de metriske kravende, men en strengt bedre enn $2$-approksimasjon løser også dominerende set korrekt.

\subsection{Metric TSP eller Traveling Salesman IRL}
TSP er et velkjent problem, arguably det mest kjente av alle NP-komplette problem. Det er vist at så mye som en $\mathcal{O}(2^n)$-approksimasjon vil implisere at $P=NP$ (ikke så veldig vanskelig å se, reduser fra hamilton sykel). Men hva om vi legger på det metriske kravet fra forrige seksjon? Det er jo overhode ikke helt unreasonable, ettersom det vil nøyaktig beskrive TSP IRL.

Vi lager en ganske straight-forward grådig algoritme, som finner starter med de to nærmeste nodene. Deretter legger den til den noden som nærmest disse to igjen, og utvider løsningen (som i Prims MSP). En fint bilde her er en strikk som strekker seg over en og en ekstra spiker, til den til slutt er innom alle noder.

Vi kan faktisk bruke denne om å bruke Prim's MSP algoritme enda mer direkte. Dette kalles "Twice Around the Tree"-algoritmen (iallfall av Hetland...). Først, se at løsningen $MSP < OPT$, siden $OPT$ er den minste Hamilton syklen (fjern en kant, så har du et spenntre. Så vi starter med å bruke Prim's grådige algoritme for å finne et MSP. Deretter dobler vi alle kanter, og lager en euler-krets. Løsningen vår er nå rekkefølgen nodene traverseres i euler-kretsen (dvs. hopp over noder når de besøkes for ikke-første gang). Dette gir en $2$-approksimasjon, fordi ved å hoppe over noder (short-cutting) blir veien iallfall ikke lengre (siden $d_{ik} + d_{kj} \geq d_{ij}$). Videre er har vi bare doblet MSPen vår så løsningen vår $C \leq 2MSP \leq 2OPT$.

En slightly bedre algoritme har navn Christofides' algoritme. Poenget med kant-doblingen isted var å få partalls grad på alle noder. Men det er litt over-kill å doble alle nodene, så isteden ser vi kun på de nodene av odde grad. For disse nodene finner vi en perfekt matching av minimal kost (det må gå, siden det finnes et partall antall av dem, fordi summen av graden til alle nodene må være partallsgrad, siden hver kant legger på 2). Denne matchingen har da kost mindre enn $\frac{OPT}{2}$. Legger vi denne matchingen oppå MSPet vårt, så vil alle noder nå ha partalls grad, og vi kan lage en eulerkrets, og løse på samme måte som før. Vi får nå $C \leq MSP + \frac{OPT}{2} \leq \frac{3}{2}OPT$, så Christofides' algoritme er en $(\frac{3}{2})$-approksimasjon.

\section{Grådighet og Matroider}
Når vi forsøker å abstrahere konseptet bak den grådige algoritmer, åpner det seg en hel ny verden, som har gitt opphave til et helt forskningsfelt, nemlig matroide-teori. For å forstå hva matroider i det hele tatt handler om starter vi med et såkalt \emph{Independent system}, eller uavhengig mengdesystem. Det består av mengde paret $(E,\boldsymbol{S})$, der $E$ er et ikke-tomt sett, og $\boldsymbol{S}\subseteq 2^E$ er lukket under inklusjon (for $A \in \boldsymbol{S} \text{ og } B \subseteq A \text{ har vi } B \in S)$. 

Dersom vi ser på $M = (E,\boldsymbol{S})$, har vi følgende ekvivalente utsagn:
\begin{itemize}
  \item $M$ er en matroide
  \item Et uavhengighetssystem med en submodulær rangfunksjon (se polymatroider)
  
  \item for alle $J,K \in \boldsymbol{S}$ med $|J| = |K| + 1$ har vi at det eksisterer en $a \in J\setminus K$, slik at $K \cup \{a\} \in \boldsymbol{S}$ 
  \item For alle $A \subseteq E$, har alle maksimale uavhengige set i $A$ samme kardinalitet.
  \item ...og mange fler
\end{itemize}
For beviser på ekvivalens se i boka (eller tenk litt, med utgangspunkt i at den grådige algoritmen alltid finner det globalt maksimale uavhengige settet i $M$). Merk at maksimale uavhengige set kalles også basiser.

Stort sett når vi ser på matroider antar vi at det underliggende problemet er maksimering, og at vi kun ser på positive vekter. Dette er uten tap av generalitet, siden dersom vi har en vekt-funksjon $w$ med negative vekter, kan vi erstatte den med en annen vekt-funksjon $w'$ der $w' = w + c$, slik at alle vekter er positive. Dette blir naturlig nok samme optimale sett som løsning, ettersom alle maksimale sett er like store ($w'(A) = w(A) + |A|c, \forall A$). Videre, om vi har et minimeringsproblem med vektfunksjon $w_1$, kan vi naturlig nok se på maksimering over vektfunksjonen $w_1' = -w_1$.

\subsection{Polymatroider og submodularitet}
I forelesning ble matroider egentlig introdusert som polymatroider der $x$ er binær. Jeg synes vel det var greiere å gå rett på matroider, men kan vel være verdifull innsikt for det. Polymatroider er igrunn de lineær-programmene som lar seg løse grådig. For å se hvilke egenskaper som da trengs, så må vi ha et bilde på en grådig algoritme for lineær-program. Det blir som følger: sorter først etter $c$ (som her betegner målfunksjonen) i synkende rekkefølge deretter øk alltid den mest verdifulle $x_j$ så mye som mulig. Så videre til kravene.

Det enkleste er å se for seg et ligningsystem på nedre triangulær form. Dersom strategien for skal fungere, må $b_i \geq b_{i+1}$ (der $i$ betegner radene). Vi kan da løse dualen på samme måte, og oppnår komplimentær slakkhet (og dermed optimalitet). Men! dessverre så har vi jo ekstremt sjeldent et ligningsystem på en slik triangulær form, så vi må opprette et krav til for restriksjonen $\boldsymbol{b}$, slik at det ikke ødelegger for strategien vår i det generelle tilfelle. Dette kravet kalles submodularitet og er som følger:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
b(a_i \cup a_j) + b(a_i \cap a_j) \leq b_i + b_j
\end{array}
\end{equation*}

I forelesning 7 er det veldig godt og visuelt forklart hvorfor dette holder. Kort forklart fordi om vi ser for oss to restriksjoner $i,j$, så kan vi flytte kolonnene i lineær programmet slik at $a_i \cap a_j$ ligger først, deretter $a_i \setminus a_j$ og tilslutt $a_j \setminus a_i$. For at optimum må finnes grådig har vi at $a_j x \leq b_j$. Videre kan vi skrive $a_j x = ((a_i \cup a_j) - a_i + (a_i \cap a_j))x$. Dermed blir kravet $b(a_i \cup a_j) - b_i + b(a_i \cap a_j) \leq b_j$, som skrevet om blir submodularitetskravet.

Et visuelt bilde på en polymatroide følger. Kort forklart og muligens litt forenklert former løsningsettet til en polymatroide en n-dimensjonal konveks polytop (n er størrelsen på lineærprogrammet i antall variabler), der optimum kan nåes ved kun å vandre langs en dimensjon (dvs. øke en variabel) av gangen! 
Et par greie egenskaper å ta med: Dersom $\boldsymbol{b}$ er heltallig, har også polymatroiden heltallshjørner. Videre gjelder det samme for snittet av to polymatroider (dette snittet er såkalt TDI, \emph{totally dual integral}). Mer om snitt av matroider under.

\subsection{Approksimering og snitt over flere matroider}
Den grådige algoritmen løser som sagt matroider optimalt. Men hva skjer om vi forsøker å benytte den grådige algoritmen over et uavhengig system som ikke nødvendigvis er en matroide? Da ønsker vi å finne en approksimasjonsgrad. 

Starten på å analysere en slik approksimasjonskrad er for en mengde $A \subseteq E$ å definere en øvre rang $ur(A)$ og en nedre rang $lr(A)$ som er definert lik kardinaliteten til henholdsvis største og minste basis (merk at for en matroide er da naturlig nok $ur(A) = lr(A), \forall A \subseteq E$. Denne kalles da bare en rang, notert ved $\rho(A)$). Dette gjør analysen av approksimasjonsgraden ganske grei. For et mengdesystem $M$, og en $A \subseteq E$, kan det være at den grådige algoritmen finner $lr(A)$, mens opt naturlig nok er $ur(A)$. For $M$ definerer vi rangkvtienten $rq(M) = min_A\{\frac{lr(A)}{ur(A)}\}$, og for et uvektet optimaliseringsproblem kan vi dermed få $\frac{APX}{OPT} \leq rq(M)$. Videre kan det vises ganske greit (den grådige algoritmen velger jo alltid de enkelt-elemntene med høyest mulig verdi osv., se boka/forelesning for et mer rigøst bevis) at for et mengdesystem vil vi også alltid ha $\frac{APX}{OPT} \geq rq(M)$.

La oss se for et mengdesystem $M = (E,S)$, der $S = \cap_{i=1}^{k}S_i$, og der $M_i = (E,S_i)$ er matroider. Dette kalles maksimering over snittet av $k$ matroider. Det kan løses eksakt for $k=2$ i polynomisk tid, men er NP-hardt for $k \geq 3$. Men, vi kan naturlig nok bruke grådighet som approksimasjon. Som vi har sett, fungerer rangkvotienten som en god nedre grense for approksimasjonsgraden, og det kan vises at $rq(M) \geq \frac{1}{k}$. Akkurat dette beviset håper jeg ikke kommer på eksamen, men detaljer finnes i forelesning 7.

\subsection{Eksempler}
Denne seksjonen har vært ganske "der ute" så langt, men med litt praktiske tilfeller og mye teori i boks, kan vi se effekten av arbeidet her i denne seksjonen. Vi starter med å se på et par ganske generelle matroider (de som er sett på i pensum):

\begin{itemize}
  \item \textbf{Grafmatroiden:} $E$ er kanter i en urettet graf, $\boldsymbol{S}$ er sett av kanter uten sykler.
  \item \textbf{Hale- og hodepartisjonsmatroider:} $E$ er kanter i en rettet graf, $\boldsymbol{S}$ er sett av kanter med ulike start- og sluttnoder.
  \item \textbf{Vektormatroider:} $E$ er en mengde med vektorer, $\boldsymbol{S}$ er lineært uavhengige mengder (dette er den ikke-algoritmiske inngangen til matroideteori).
  \item \textbf{Uniforme k-matroider:} $E$ er kanter i en mengde, $\boldsymbol{S}$ er delmengder opp til kardinalitet k.
  \item \textbf{Transversalmatroider:} $E$ er venstrenoder i en bipartitt graf, $\boldsymbol{S}$ er sett av noder som kan matches samtidig.
\end{itemize}
Ettersom disse mengdesystemene er matroider, vet vi nå at alle disse maksimeringsproblemene kan løses optimalt med den grådige algoritmen (eksempelvis er den grådige algoritmen over grafmatroiden enkelt og greit Prim's algoritme med negativ vekt-funksjon). 

Videre kan vi endelig forstå den fantastiske setningen fra seksjon 1! Snittet av to transversal-matroider er naturlig nok mere kjent som problemet matching i en bipartitt graf (en matroide for hver side av den bipartitte grafen). Dermed vet vi at den grådige algoritmen løser bipartitt matching med approksimasjonsgrad $\alpha = \frac{1}{2}$. Videre vet vi også at det kan løses eksakt i polynomisk tid, siden det er snittet over $k=2$ matroider. Ellers kan vi egentlig like lett finne approksimasjonsgraden til den grådige algoritmen for matching i generelle grafer. Ved å starte med å gi enhver kant i grafen en vilkårlig retning, ser vi at snittet av hale- og hodepartisjonsmatroider for grafen vil tilsvare matchinger i den original grafen. Dermed vet vi at den grådige algoritmen faktisk løser generelle matchinger med approksimasjonsgrad $\alpha = \frac{1}{2}$.

Et annet eksempel er spenntrær i en rettet graf. Det kan modelleres som snittet av grafmatroiden og hode- ELLER halepartisjoner, og viser oss at dette problemet kan løses i polynomisk tid, selvom den grådige algoritmen ikke gir optimal løsning. Videre kan vi betrakte snittet av grafmatroiden og hode- OG halepartisjoner, og med litt tenking ser vi fort at de maksimale uavhengige mengdene her må tilsvare hamiltonstier. Dette viser at optimalisering over snittet av $k \geq 3$ matroier er NP-hardt, men at den grådige algoritmen løser det tilsvarende optimaliseringsproblemet med approksimasjonsgrad $\alpha = \frac{1}{3}$. Så mye approksimeringsteori! Og så mye enklere enn tidligere nå som vi kan anvende matroider! Fantastisk.

\subsection{"Greedoids"}
Helt til slutt kan det nevnes at kravet om inklusjon i uavhengige mengdesystemer faktisk er litt for strengt i forhold til hva som trengs for å kunne løse et optimaliseringsproblem med den grådige algoritmen. Dermed kan vi bytte ut det kravet heller med kravet at dersom $A \in \boldsymbol{S}$, så finnes det alltid en $e \in A \text{ slik at } A \setminus \{e\} \in \boldsymbol{S}$. Legg merke til at dette kravet, kalt tilgjengelighet, er mye mindre strengt enn kravet om inklusjon. Et slikt tilgjengelig system er en "greedoid" (en generalisering av matroide) hvis og bare hvis kravet for matroider i uavhengige mengdesystemer "for alle $J,K \in \boldsymbol{S}$ med $|J| = |K| + 1$ har vi at det eksisterer en $a \in J\setminus K$, slik at $K \cup \{a\} \in \boldsymbol{S}$" utvides til at også $J \setminus \{a\} \in \boldsymbol{S}$ (i uavhengige mengdesystemer var dette ikke et nødvendig krav, siden for alle $A \subseteq J \text{ hadde vi } A \in \boldsymbol{S}$). Legg også merke til at $\emptyset \in \boldsymbol{S}$ for alle tilgjengelige systemer så vel som alle uavhengige mengdesystemer.

\section{Flyt}
Flyt brukes mye i modellering, og har vist seg å være en av de viktigste teori-områdene for å modellere praktiske problemer. Vi starter med litt bakgrunn som vanlig, og så ser vi på litt praktiske problemer som kan løses ved flyt. Flyt burde være litt kjent fra algdat. Videre har vi alt sett at flyt som et lineærprogram har tilhørende dual minste s-t kutt.

\subsection{Bakgrunn}

Et flytnettverk er en graf $G = (V,A)$, med kapasiteteter på kantene $u(i,j)$, samt to markerte noder $s,t \in V$, en kilde $s$ og et sluk $t$. Videre må flyten følge to krav, nemlig kravet "capacity constraint", at $\forall (i,j) \in A, 0 \leq f(i,j) \leq u(i,j)$, samt kravet "flow conservation", at $\forall i \neq s,t, \sum\nolimits_{k:(i,k)\in A}f(i,k) = \sum\nolimits_{k:(k,i)\in A}f(k,i)$. Verdien på flyten $|f|$ er da $|f| = \sum\nolimits_{k:(s,k) \in A} f(s,k) - \sum\nolimits_{k:(k,s) \in A} f(k,s)$ (flyt ut av kilden minus flyt inn til kilden), eller ekvivalent (pga. flow conservation) $\sum\nolimits_{k:(k,t) \in A} f(k,t)$. Målet er da generelt å finne størst mulig $|f|$.

Mye av den teoretisk analysen baserer seg på såkalt "skew symmetry", hvor vi $\forall (i,j) \in A$ setter $f(j,i) = -f(i,j), u(j,i) = 0$. Det er lett å se at vi fremdeles opprettholder kravene om flyt når vi legger på skew symmetry. Faktisk kan vi nå sette definere flow conservation enda enklere som $\forall i \neq s,t, \sum\nolimits_{k:(i,k)\in A}f(i,k) = 0$. Tilsvarende setter vi $|f| = \sum\nolimits_{k:(s,k) \in A}f(s,k)$.

Vi har tidligere vært innom notasjonen $\delta (S)$ for et s-t kutt $S$. Som nevnt er minste mulige $\delta (S)$ dualen til lineærprogrammet for flytnettverk, og dermed får vi med en gang at $|f| = min_S \delta (S)$. Det kan også vises direkte uten kunnskap om lineærprogram ganske greit, se forelesning eller boka. Det direkte beviset viser i første omgang at $|f| \leq \delta (S), \forall S$. For likhet må vi ha et konsept om Residual grafen. Den er definert for en flyt $f$ på en graf $G = (V,A)$ med kapasiteter $u(i,j)$ som $G_f = (V,A)$, der $u_f(i,j) = u(i,j) - f(i,j)$. Dersom det finnes en path $P$ på $G_f$ fra $s$ til $t$ kalles dette en augmenting path, og eksistensen av en slik $P$ impliserer at $|f|$ ikke er maksimal (fordi vi kan da dytte $min_{(i,j) \in P} u_f(i,j)$ enheter langs $P$). Med denne forkunskapen kan vi bevise at følgende uttrykk er ekvivalente:

\begin{itemize}
  \item $f$ er en maksimal flyt
  \item Det finnes ingen augmenting path $P$ i $G_f$
  \item $|f| = u(\delta (S))$ for et min. s-t kutt $S$
\end{itemize}

Bevisene her er ikke så ille, se evt. boka/forelesning. En til egenskap som er viktig å nevne er heltalls-egenskapen ved flyt, som sier at dersom alle kapasiteter $u(i,j)$ er heltallige, vil også den maksimale $f$ være heltallig. Dette er kjempenyttig, siden da holder det alltid å vise at en ikke-heltallig $f'$ eksisterer, og dermed eksisterer en heltallig $f$ med $|f| \geq |f'|$. Dette er ikke en veldig vanskelig egenskap å se, kan f.eks vises ved å finne den maksimale $f$ ved å kun bruke augmenting paths (disse må jo alltid være heltallige).

\subsection{Praktiske eksempler}
Vi ser raskt (litt slurvete) på tre praktiske problemer som kan modelleres med flyt, og håper at disse inspirerer til å løse lignende oppgaver på eksamen.

\subsubsection{Carpool Driver}
Over en periode på $n$ dager skal $m$ forskjellige mennesker sette opp en carpool-kalender. Hver av dagene er $k$ folk med i bilen. Tanken er da at disse er ansvarlige for å kjøre $\frac{1}{k}$-del av bilen. La $r_i$ være det totale ansvaret person $i$ fått på seg. Målet er at person $i$ skal maks måtte kjøre $\lceil r_i \rceil$ ganger. Løses ved å sette opp en rad med noder for alle personene, og kanter fra $s$ til person $i$ med kapasitet $\lceil r_i \rceil$. Videre, sett opp en rad med noder for alle dagene, og en kant fra hver person til de dagene den personen skal sitte i bilen med ubegrenset kapasitet. Tilslutt sett kanter fra dagene til $t$ med kapasitet 1.

En liten sidenote her er at dette alltid er løslig, siden det er helt trivielt løslig fraksjonelt, og pga. heltallsegenskapen (ubegrenset kapasitet kan bestemmes å være heltallig) får vi at dette er heltallig-løslig.

\subsubsection{Baseball Elimination Problem}
Litt mye notasjon å skrive opp her som kun er relevant for akkurat dette problemet, så outliner det heller litt grovt, og overlater beviser til boka. Vi ønsker å "eliminere" lag fra en baseball-liga (eliminere betyr i denne settingen at de ikke lenger kan vinne), og setter det opp ved å lage et flytnetthverk for hvert lag $k$ vi ønsker å skjekke om er eliminert. Det kan gjøres ved å sette en rad med noder som representerer alle de forskjellige parene av lag i ligaen utenom $k$, og en kant fra $s$ til disse med kapasitet "gjenværende kamper mellom disse to". Deretter en kant fra dette paret til hvert av lagene i paret med uendelig kapasitet. Til sist, en kant fra alle lag $i$ til $t$ med kapasitet "antall mulige poeng for $k$ (seiere + gjenværende spill) minus seiere for lag $i$". Dersom $\{s\}$ nå er et min s-t kutt, er ikke laget eliminert. En grei intuisjon for dette er at vi klarer å dytte en flyt tilsvarende gjenstående kamper mellom alle andre lag enn $k$ gjennom nettverket uten at noen får flere seiere enn $k$ teoretisk kan få.

\subsubsection{Maximum Density Subgraph}
Ganske generelt graf-problem, gitt en graf $G = (V,E)$, finn det settet $S \subseteq V$ slik at grafen indusert av $S$, $G(S) = (V,E(S))$ har høyest mulig "tetthet", $D = \frac{|E(S)|}{|S|}$. Det nøyaktige oppsettet her leder til litt knotete notasjon og en del bokføring for bevis av korrekthet osv., så hopper greit over det, kan evt. se i boken/forelesning. Men heller er det kule konseptet her idéen om å bruke maks-flyt + binærsøk for å finne svaret, når maks-flyt modellen kan brukes som et orakel for å skjekke om en parameter er en gyldig løsning. Litt mer formelt, om $D^*$ er den maksimale tettheten, vet vi at $D^*$ er i intervallet $[0,m], m = |E(S)|$. Deretter kan sette $\gamma = \frac{(l + u)}{2}$, der $l,u$ betegner nedre og øvre grense i intervallet, og benytte maks-flyt oppsettet vårt til å skjekke om $\gamma$ er lavere enn den maksimale tettheten. Hvis den er lavere blir det nye intervallet $[\gamma,u]$, ellers blir det $[l,\gamma]$. Deretter, siden $D^*$ er rasjonal, med begrenset størrelse på nevneren (maksimal størrelse er jo $n = |V|$), vet vi at ved å utføre binærsøket et endelig antall ganger ($\mathcal{O}(log(n))$) har vi begrenset intervallet til kun ett mulig tilfelle.

\subsubsection{Project Planning}
Oppgaven er tatt fra øving 4. Her har vi tre prosjekter, P1, P2 og P3 som krever så så mange "arbeidsmåneder" å fullføre, og som bare kan utføres under visse måneder, 4 måneder å utføre dem på og 8 arbeidere. Videre kan kun 6 arbeidere jobbe på hvert prosjekt pr. måned. Vi setter ganske greit opp problemet ved å ha kanter fra $s$ til arbeiderne med uendelig kapasitet. Deretter en kant fra arbeiderne til hver måned med kapasitet 1. Så en kant fra hver måned til hvert prosjekt, tilsvarende de månedene prosjektene kan utføres på, alle med kapasitet 6. Til slutt en kant fra alle prosjektene til $t$ med kapasitet tilsvarende antall arbeidsmåneder det krever å fullføre prosjektet.

\subsubsection{k-Orientering}
Tatt fra konteksamen 2018. Oppgaven lyder som følger "En orientering av en urettet graf $G=(V,E)$ er en tilordning av retning til hver kant $e \in E$, som resulterer i en ny rettet graf $D = (V,A)$, der hver urettet kant
$(u,v) \in E$ tilsvarer en rettet kant enten $(u,v) \in A$ eller $(v,u) \in A$. Videre er en $k$-orientering en orientering der hver node $v$ har maksimalt $k(v)$ inn-kanter, for en funksjon $k:V \longrightarrow \mathbb{N}$. Hvordan kan du effektivt finne en $k$-orientering v.hj.a. maks flyt?".

Dette løses ganske greit ved å sette opp en rad med noder for hver kant $e \in E$, og en kant fra $s$ til denne raden med kapasitet 1. Deretter sett opp en rad med noder for hver node $v \in V$, og legg til en kant mellom disse radene slik at hver kant kobles med sine ende-punkter i $G$. Disse kantene kan ha kapistet 1 (eller uendelig, spiller ingen rolle). Til slutt sett opp en kant fra hver node $v$ til $t$ med kapasitet $k(v)$.

\subsection{Most improving / Shortest Augmenting Path}
Så langt har vi jo faktisk ikke gått veldig inn på noen algoritme for å løse flyt-problemet, selvom vi i introduksjonen hintet litt til å bruke disse augmenting path'ene. Problemet her er at ved å ta en tilfeldig augmenting path, får vi ikke en polynomisk kjøretid. Hver augmenting path øker flyten med minimum 1, så vi trenger maksimalt $\mathcal{O}(mU)$ slike paths, hvor $U = max(u(i,j))$. En kjøretid på $\mathcal{O}(mU)$ er kun \textbf{pseudo-polynomisk}, siden numerisk data, i dette tilfelle $u(i,j)$, er som regel inputtet i binær. Med andre ord trenger vi $log(U)$ bits for å inpute $U$, og kjøretiden blir dermed eksponensiell. Vi viser straks en algoritme som har polynomisk kjøretid. Videre er det mulig å oppnå det som kalles \textbf{Strongly polynomial}, dvs. at kjøretiden kun er avhengig av antall elementer i inputen, og ikke størrelsen av dem (vi ser på det og).

En naturlig idé er å finne den mest forbedrene ("most improving") augmenting pathen i $G_f$. For å analysere en slik algoritme, bruker vi veldig grei teori om flyt-dekomponering, $f = f' + f''$. Det er lett å bevise at summen av to flyt $f' og f''$ blir en ny flyt som overholder både capacity constraint og flow conservation, og at $|f| = |f'| + |f''|$. Se boka/forelesning for detaljer. Videre har vi at viktig lemma som sier at for enhver s-t flyt $f$ så finnes det flyter $f_1, f_2,...,f_l, l\leq m$ slik at $f = \sum\limits_{i=1}^{l}f_i$, hvor for alle $i$, har vi at de kantene med positiv flyt i $f_i$ enten danner en s-t path eller en sykel. Dette lemma kan vises med induksjon. Videre følger ganske naturlig gitt en maksimal flyt $f^*$ og en hvilken som helst s-t flyt $f$, så har den maksimal flyten i $G_f$ verdi $|f^*| - |f|$, og videre fra det første lemma kan denne maks-flyten i $G_f$ bygges av augmenting paths, der den most improving augmenting pathen har verdi minst $\frac{1}{m}(|f^*|-|f|)$. Dermed ved å alltid velge den "most improving" får vi etter k iterasjoner $|f^*| - |f|^{(k)} \leq (1-\frac{1}{m})^k(|f^*|-|f|)$, som etter $k = mlog(mU)$ vil gi oss maks flyten, altså har vi en polynomisk kjøretid (f.eks. gitt en ganske naiv most improving path algoritme med $\mathcal{O}(m^2)$ kjøretid, oppnår vi en total kjøretid på $\mathcal{O}(m^3log(mU)$. Denne kan forbedres litt ved enten en bedre "most improving" algoritme, eller å istedenfor å alltid finne den most improving augmenting path'en, nøyer vi oss med en path som er "bra nok", eller som har verdi mer enn $\Delta$.

Som lovt er det og mulig å oppnå en strongly polynomial kjøretid, altså helt uavhengig av $U$. Ideen er veldig lite revolusjonerende, men analysen blir veldig bra. Vi velger isteden alltid den korteste augmenting pathen. Analysen på kjøretiden her går på å se på at hver kant kan kun bli saturert ($f(i,j) = u(i,j)$) $\mathcal{O}(n)$ ganger (dette gjøres ved å sette på distanse labels osv. osv., se neste seksjon for mer info), og videre siden det kun eksisterer $m$ kanter får vi at vi må finne $\mathcal{O}(mn)$ slike paths. Korteste vei kan finnes i $\mathcal{O}(m)$ tid (ikke trivielt!), så totalt får vi da en kjøretid på $\mathcal{O}(m^2n)$.

\newpage
\section{Preflyt}
De forrige algoritmene brukte på et eller annet vis alle augmenting paths for å finne maksimal flyt. Men ved kun å bruke slike paths kan vi få ganske dårlig kjøretid ved å konstruere spesifikke eksempler (der vi trenger mange augmenting paths, som alle kun øker flyten med 1). I denne seksjonen skal vi se en litt annen maks-flyt variant, som unngår dette problemet, og som er en av de raskere maks-flyt algoritmene idag, både teoretisk og praktisk.

\subsection{Push-Relabel}
Den algoritmen baserer seg på distance labels (så vidt nevnt i slutten av forrige seksjon), og såkalt preflyt. En preflyt har samme krav om capacity constraint samt skew symmetry fra flyt, men har et løsere krav enn flow conservation: nemlig at for alle $i \in V, i \neq s$ så har vi $\sum\nolimits_{k:(k,i)\in A}f(k,i) \geq 0$ (sagt med ord at det kommer mer flyt inn enn det går ut av hver node, husk vi har skew symmetry). Vi kaller denne mengden \emph{excess at $i$ for a preflow $f$}, og skriver $e_f(i)=\sum\nolimits_{k:(k,i)\in A}f(k,i)$. Legg også merke til at en flyt er og en gyldig preflyt.

Videre ser vi på distance labels $d(i), \forall i \in V$. Disse representerer på et vi en nedre grense for distansen til sluket. En intuisjon som blir nyttig etterhvert er og at de representerer "høyden" til node i. Vi har en gyldig distance labeling om følgende krav er oppfylt:

\begin{itemize}
  \item $d(s) = n$
  \item $d(t) = 0$
  \item $d(i) \leq d(j) + 1, \forall (i,j) \in A_f \text{(residualgrafen)}$
\end{itemize}

Videre har vi at for en preflyt $f$ og en valid distance labeling $d$, finnes ingen augmenting paths i $G_f$. For å se dette, anta at det finnes en s-t path $P$ i $G_f$ (altså en augmenting path). P inneholder maks $n$ noder, og dermed maks $n-1$ kanter. Da får vi $d(s)=d(t) + |P| \leq 0 + n-1 = n-1$. Men $d(s)=n$, så dette er en kontradiksjon. 

Dette gir oss alt vi trenger for å outline push-relabel algoritmen. Vi starter med en valid distance labeling og en preflyt. Deretter forsøker vi gradvis å gjøre om preflyten til en flyt, mens vi holder distance labelingen valid. Dette resulterer da i en maks flyt på grunn av resultatet over.

Litt mer spesifikt ønsker vi å dytte så mye \emph{excess at i} mot sluket, over de kantene som er på den \emph{tilsynelatende} korteste veien. Med det menes at vi kun dytter flyt over en kant $(i,j)$, dersom $d(i) = d(j) + 1$ (om dette og $u_f(i,j) > 0$ holder, kaller vi kanten admissable). Når vi dytter dytter vi alltid mest mulig, dvs. $min(e_f(i),u_f(i,j))$. Om vi ikke har noen admissable kanter for en node $i$ med $e_f(i) > 0$, da tolker vi det som at labelen er feil, og gjør en relabel. Mer spesifikt øker vi $d(i)$ til $d(i) = min_{j:(i,j) \in A_f}(d(j)+1)$, dvs. den laveste $j$ som $i$ har residual-flyt til. Merk at en slik $j$ alltid finnes, pga. skew symmetry kan vi alltids dytte flyt tilbake igjen. Her er $d(i)$ som høyde en god intuisjon, der flyt alltid går nedover.

Om vi nå initialiserer push-relabel algoritmen vår som $d(s)=n, d(i)=0, i \neq s$, og setter $f(s,k) = u(s,k) \forall k$, så ser vi at algoritmen holder en gyldig preflyt og en valid distance labeling hele veien. Dette er ikke vanskelig å se, se evt. boka/forelesning. Det er og lett å se at når algoritmen stopper (da $e_f(i) = 0, \forall i \neq s$) har vi en flyt, og som argumentert for tidligere er den maksimal.

\subsection{Kjøretid}
Analysen av kjøretiden baserer seg hovedsaklig på to resultater. Den ene sier at for enhver preflyt $f$, for en node $i, e_f(i) > 0$, så finnes det alltid en path fra i til s i $G_f$. Det andre resultatet er at for en node $i$ har vi $d(i) \leq 2n-1$. Dette fordi at for at vi skal relable så må $e_f(i) > 0$, og da finnes det en path $P$ til $s$. Dette gir at $d(i) \leq d(s) + |P| \leq 2n - 1$.

Se nå på antall relabel operasjoner. På starten har vi $d(i) = 0, \forall i\neq s$. Videre er som vist alltid $d(i) \leq 2n - 1$. Hver relable øker $d(i)$ med minst 1. Siden vi aldri endrer på $s,t$ har vi $n-2$ noder som kan relables. Da får vi et maks antall relable operasjoner som $(n-2)(2n-1) = \mathcal{O}(n^2)$. Analysen av antall pushes er hakket mer tricky, men fortsatt greit. Vi deler inn i saturerende ($u(i,j)$ blir dyttet) og ikke-saturerende (mindre enn $u(i,j)$ blir dyttet) pushes. Samme som for i shortest path har vi en øvre grense på $\mathcal{O}(mn)$ ganger å saturere kanter. For å se det, se at for at en path $(i,j)$ skal satureres 2 ganger, må $d(i)$ øke med minst to, men vi har og $d(i) \leq 2n-1$. Analysen av antall ikke-saturerende pushes er smart. Vi konstruerer en funksjon $\phi = \sum\nolimits_{i active}d(i)$. Den slutter som 0, og er aldri negativ. Det sentrale er at et ikke-saturerende push senker $\phi$ med minst 1. For å se dette, se at selv om pushet gjør $j$ aktiv hadde vi jo $d(i) = d(j)+1$ (ikke-saturerende betyr jo at vi dytter $e_f(i)$, så $i$ blir jo nødvendigvis inaktiv). Dermed blir en øvre grense for ikke-saturerende dytt gitt av hvor mye $\phi$ kan øke med under algoritmen. Ved relabeling kan vi øke $\phi$ med $\mathcal{O}(n^2)$, siden vi har $n-2$ noder som kan økes med $2n-1$. Videre vil også saturerende push øke $\phi$, med opp til $2n-1$ ($i$ blir ikke nødvendigvis inaktiv, mens $j$ kan bli aktiv). Som vi har sett kan vi ha $\mathcal{O}(mn)$ slike dytt og vi får en øvre grense for saturerende dytt på $\mathcal{O}(n^2m)$ Dermed blir også en øvre grense for ikke saturerende dytt $\mathcal{O}(n^2m)$. Alle operasjoner tar $\mathcal{O}(1)$ tid, så total kjøretid på push-relabel algoritmen blir dermed også $\mathcal{O}(n^2m)$.

\subsection{Forbedringer}
Vi har en forbedrert versjon av push-relabel kalt highest label push-relabel, som forbedrer kjøretiden til $\mathcal{O}(n^2\sqrt{m})$, der vi alltid dytter fra den aktive noden $i$ med høyest $d(i)$. Kjøretidsanalysen her er hakket mer teknisk, men går frem stort sett på likt vis som i forrige avsnitt.

Vi har også små forbedringer, som ikke forbedrer den teoretiske kjøretiden, men som fremdeles kan bety mye for den praktiske kjøretiden. Det første er at vi tidlig kan finne $|f|$ uten å vite $f$ ($f$ betegner her den maksimale flyten). Den baserer seg på å redefinere aktive noder til kun å være aktive hvis de også har $d(i) < n$. Dette gir $|f|$ på grunn av et resultat som sier at "om vi terminererer push-relabel algoritmen når $e_f(i) > 0$ impliserer $d(i) \geq n, i \neq t$, da er settet $S$ av noder som ikke kan nå $t$ i $A_f$ et min s-t cut". Se detaljer i boka/forelesning. Et annet raskt triks da er "gap relabeling", som sier at om det opstår et gap i distance labelsene, typ en $k < max_i(d(i))$, så setter vi $d(j) = n, \forall j:k < d(j) < n$. Et siste triks som eksperimentelt har vist seg å være lurt å gjøre av og til (ca. hver $n$ relabel) er å regne faktisk distanse til $t$ for alle noder, og relable $d(i)$ til den korrekte distansen.

\end{document}