%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{
    \documentclass[twoside,11pt]{article}
    %%%%% PACKAGES %%%%%%
    \usepackage{pgm2016}
    \usepackage{amsmath}
    \usepackage{algorithm}
    \usepackage[noend]{algpseudocode}
    \usepackage{subcaption}
    \usepackage[english]{babel}	
    \usepackage{paralist}	
    \usepackage[lowtilde]{url}
    \usepackage{fixltx2e}
    \usepackage{listings}
    \usepackage{color}
    \usepackage{hyperref}
    \usepackage{amssymb}
    
    \usepackage{auto-pst-pdf}
    \usepackage{pst-all}
    \usepackage{pstricks-add}
    
    %%%%% MACROS %%%%%%
    \algrenewcommand\Return{\State \algorithmicreturn{} }
    \algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}
    \renewcommand{\thesubfigure}{\roman{subfigure}}
    \definecolor{codegreen}{rgb}{0,0.6,0}
    \definecolor{codegray}{rgb}{0.5,0.5,0.5}
    \definecolor{codepurple}{rgb}{0.58,0,0.82}
    \definecolor{backcolour}{rgb}{0.95,0.95,0.92}
    \lstdefinestyle{mystyle}{
       backgroundcolor=\color{backcolour},  
       commentstyle=\color{codegreen},
       keywordstyle=\color{magenta},
       numberstyle=\tiny\color{codegray},
       stringstyle=\color{codepurple},
       basicstyle=\footnotesize,
       breakatwhitespace=false,        
       breaklines=true,                
       captionpos=b,                    
       keepspaces=true,                
       numbers=left,                    
       numbersep=5pt,                  
       showspaces=false,                
       showstringspaces=false,
       showtabs=false,                  
       tabsize=2
    }
    \lstset{style=mystyle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }

%%%%%%%%%%%%%%%%%%%%%%%% CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% {
\newcommand\course{CS 330-001}
\newcommand\courseName{Introduction to Operating Systems}
\newcommand\semester{Winter 2020}
\newcommand\assignmentNumber{1}                             % <-- ASSIGNMENT #
\newcommand\studentName{Your Name}                  % <-- YOUR NAME
\newcommand\studentEmail{email@uregina.ca}          % <-- YOUR NAME
\newcommand\studentNumber{200XXYYZZ}                % <-- STUDENT ID #
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{


    \firstpageno{1}
    
    \begin{document}
    
    \title{AlgKons(entrert) Kompendium}

    \maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }
\tableofcontents
\newpage
\section{Matching}

\newpage
\section{Lineær Programmering}
Dette er kanskje det viktigste konseptet i faget, og grunnlaget for omtrent hele første halvdel av pensum. Lineære programmer er en veldig generell måte å modellere/sette opp optimaliseringsproblemer over $\mathbb{Q}^n$, der løsningen består av å optimalisere en lineær målfunksjon, samtidig som vi holder oss innen lineære restriksjoner. Lineær program har en veldig visuel løsningsform, der restriksjonene tilsammen lager en (konveks) polytop av gyldige løsninger, og målfunksjonen bestemmer retningen hvor den optimale løsningen ligger. Dessverre så kan også restriksjonene være umulige, eller ubegrenset. Men dersom det ikke gjelder, kan vi generelt finne den optimale løsningen i polynomisk tid!

\subsection{Kanonisk Form}

Generelt ser den kanoniske formen på lineær program slik ut:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \boldsymbol{c}^T &\boldsymbol{x}&\\
\text{subject to:}& A&\boldsymbol{x} \leq \boldsymbol{b}\\
&&x_i \geq 0&\forall x_i \in \boldsymbol{x}
\end{array}
\end{equation*}

Her er $A$ en matrise, mens $w,x,b$ er vektorer. Grunnen til at en slik "kanonisk form" er mulig er ganske enkelt fordi alle lineærprogram kan omformuleres til denne formen. Det gjøres slik: $min(cx) = max(-cx), x \geq b \iff -x \leq b, x = b \iff x \leq b \wedge x \geq b$. Merk at både gjennom boka og kompendiet så brukes $min,max$ om hverandre (begge er definert som kanonisk form). Dette fordi at enkelte problem er naturlig å tenke på som maksimeringsproblemer, mens andre er mer naturlig å tenke på som minimeringsproblemer. Uansett, det er ønskelig å få alle ulikhetene samme vei for restriksjonene, så dette gjøres stort sett også.

\subsection{Heltallsprogram}
Veldig mange praktiske (og teoretiske) problemer har en form for heltallighet som krav. Hvis $\boldsymbol{x}$ representerer antal av forskjellige komponenter i en optimal løsning, er det naturlig å tenke seg at alle $x_i \in \boldsymbol{x}$ må være heltallige, avhengig av optimaliseringsproblemet (hvis $\boldsymbol{x}$ representerer mennesker fra forskjellige grupper f.eks. så er det jo uheldig å få en fraksjonell løsning). Dermed kan dette være en naturlig restriksjon på lineærprogrammet vårt. Da ender vi opp med det som kalles et heltallsprogram. Fra nå av bruker vi IP og LP (IP = Integer Program, LP = Linear Program). En annen variant av IP kan modelers som at $x_i \in \{0,1\}$. Dette gir en veldig naturlig måte å formulere "decision"-type problemer, der $0,1$ representerer valg (typisk ta med i løsningen eller ikke). Som vi vet er slike "decision"-problemer veldig ofte NP-harde, noe som betyr at det å løse et IP er generelt NP-hardt også.

Likevel så er det flere ting vi kan gjøre for å jobbe rundt dette, og heltallsprogram er den absolutt sentrale delen av pensum, og helt håpløst er det ikke. For det første har enkelte lineær-program har alltid heltallige løsninger (selvom det ikke har heltallskrav), og for det andre kan ofte løsningen på slakkingen av IPet fortelle oss noe om løsningen (hvis vi klarer å argumentere for en øvre grense på forskjellen mellom løsningen på IPet og det tilhørende LPet). En slakking av et heltallsprogram er når vi fjerner heltallskravet og ender opp med et tilhørende lineærprogram.

\subsection{Dualitet}
Også et utrolig viktig konsept som går igjen i første halvdel av pensum (mantraen er generelt lineærprogram++ går igjen i første halvdel...). For ethvert lineærprogram, kan vi enkelt konstruere det såkalte "duale" lineærprogrammet. Dette omtales som DUALEN, mens det originale lineærprogrammet da omtales som PRIMALEN. Vi lager dualen ved å sette en variabel for hver restriksjon i primalen, og en restriksjon i dualen for hver variabel i primalen. En intuitiv forståelse av dualen er litt vanskelig å skrive ned, men det kommer når det jobbes med. Det som er målet er å finne en øvre grense for den optimale løsningen for primalen, og det viser seg at denne øvre grensen også er et lineær program. Dualen til den kanoniske formen nevnt tidligere ser slik ut, og forsøker å skvise en øvre grense så nært som mulig:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Mimimize: }  & \boldsymbol{b}^T &\boldsymbol{y}&\\
\text{subject to:}& A^T&\boldsymbol{y} \geq \boldsymbol{c}\\
&&y_i \geq 0&\forall y_i \in \boldsymbol{y}
\end{array}
\end{equation*}

Hadde vi tatt dualen av dette lineær-programmet igjen, hadde vi endt opp med den kanoniske formen vi starta med. Dette er generelt også alltid sant (Dualen til DUALEN er igjen PRIMALEN). 

For gyldige løsninger på primalen og dualen kan vi generelt skrive følgnde ulikheter

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Primal: }  & &A\boldsymbol{x} \leq \boldsymbol{b}&\\
\text{Dual:}& \boldsymbol{c} \leq \boldsymbol{y}&A\\
x,y \geq 0 &\boldsymbol{c}\boldsymbol{x} \leq \boldsymbol{y}&A\boldsymbol{x} \leq \boldsymbol{y}\boldsymbol{b}&\\
\text{Ved optimum (!):}&\boldsymbol{c}\boldsymbol{x} = \boldsymbol{y}&A\boldsymbol{x} = \boldsymbol{y}\boldsymbol{b}&\\
\end{array}
\end{equation*}
Den nest nederste kalles ulikheten kalles svak dualitet, mens den nederste likheten kalles sterk dualitet. Beviset for sterk dualitet kan sees i boken.

Dette gjelder naturlig nok kun når både primalen og dualen er endelig optimale (det holder å vise at en av dem er det, for da må også den andre være det). Generelt finnes det 4 tilfeller, som jeg lister raskt opp nå; 1) PRIM endelig optimal, DUAL endelig optimal, 2) PRIM ubegrenset, DUAL umulig, 3) PRIM umulig, DUAL ubegrenset, 4) begge umulig. 

\subsection{Komplementær slakkhet}
Et viktig teorem, basis for primal-dual metoden som det står skrevet om litt lenger nede. Komplementær slakkhet er følgende "betingelse" for et PRIMAL/DUAL par:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
y_i = 0 \Longleftarrow (Ax)_i \neq b_i & \forall i\\
x_j = 0 \Longleftarrow (yA)_j \neq c_j & \forall j\\
\end{array}
\end{equation*}

Det er lett å se at komplementær slakkhet impliserer likhet mellom målfunksjonene (at det er en tilstrekkelig betingelse for likhet). Men at kanskje noe mer overraskende resultat er at komplimentær slakkhet også er nødvendig for å oppnå likhet! Dvs. likhet mellom målfunksjonene impliserer komplimentær slakkhet. Her er et pent bevis:

Se for oss at vi har likhet mellom målfunksjonene, altså $\boldsymbol{c}\boldsymbol{x} = \boldsymbol{y}A\boldsymbol{x} = \boldsymbol{y}\boldsymbol{b}$. Den siste halvdelen kan vi skrive om som $\boldsymbol{y}(\boldsymbol{b}-A\boldsymbol{x}) = 0$, og vi vet at $\boldsymbol{b}-A\boldsymbol{x} \geq 0$, siden restriksjonen var at $A\boldsymbol{x} \leq \boldsymbol{b}$. Dette gir oss følgende:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\boldsymbol{y}(A\boldsymbol{x}-\boldsymbol{b}) = &\sum\limits_{i=1}^{n}y_i(Ax-b)_i = 0\\
& (Ax-b)_i \geq 0 & \forall i \in \{1,..,m\}\\
& y_i \geq 0 & \forall i \in \{1,..,m\}
\end{array}
\end{equation*}

og det er helt tydelig at vi må ha komplimentær slakkhet (dette viser strengt tatt bare komplimentær slakkhet den ene veien, men et helt symmetrisk argument gir beviset andre veien og).

\newpage

\section{Den ungarske metoden}

\newpage

\section{Approksimasjon}
\label{sec:approx}


Approksimasjonsalgoritmer er algoritmer for å løse optimaliseringsproblemer. Konseptet som introduseres er litt løst basert på følgende utsagn: For å løse optimaliseringsproblemer trenger man 1) Optimal løsning, 2) I rask tid, 3) for alle problemer. Velg 2. Approksimeringsalgoritmer slacker på kravet om optimal løsning, ved å heller gi et svar som er så nært som mulig optimalt, men ikke nødvendigvis optimalt.

For å få noe av verdi må vi kunne si noe om hvor nære approksimasjonen er, eller approksimasjonsgraden. La APX og OPT være hhv. approksimasjonen og den optimale løsningen. Da sier vi at algoritmen er en $\alpha$-approksimasjon dersom.

\begin{align} \label{eq_01}
	&APX \geq \frac{1}{\alpha}OPT \text{ For maksimeringsproblemer}\\ 		
	&\frac{1}{\alpha}APX \leq OPT \text{ For minimeringsproblemer}
\end{align}

Et viktig konsept som går igjen er PTAS vs FPTAS. Definisjonen på en PTAS er en familie med approksimeringsalgoritmer $\{A_\epsilon\}_{\epsilon>0}$, der alle $A \in \{A_\epsilon\}_{\epsilon>0}$ er ($1 + \epsilon$)-approksimasjoner, med polynomisk kjøretid i n. En FPTAS er en PTAS der kjøretiden også er polynomisk i $\frac{1}{\epsilon}$. For å illustrere forskjellen oppfyller $O(n^{\frac{1}{\epsilon}})$ kun kravet til PTAS, mens $O(n^c + (\frac{1}{\epsilon})^d)$ oppfyller kravet til FPTAS også.

\subsection{Maximum Independent set/Største Uavhengige Set}
For en graf $G = (V,E)$, kalles en $I\subseteq V$ independent/Uavhengig dersom ingen to noder i $I$ er koblet sammen.

Å finne det største kan skrives som et lineærprogram på følgende måte

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\nolimits_{v\in V} &x_v &\\
\text{subject to:}&x_u + &x_c \geq 1,  &\text{ for alle kanter $(u,v)\in E$}\\
                 &                                                &x_v \in \{0,1\}, &\text{ for alle noder $v\in V$}
\end{array}
\end{equation*}

Dette er hovedsaklig tatt med som et eksempel på problemer som ikke kan approksimeres (Ikke 100\% sikker på dette, men tror det er et teorem om at det ikke finnes noen PTAS for største uavhengige sett, med mindre $P = NP$). Det samme gjelder for Maksimum Clique problemet (Hvordan disse problemene er relatert overlater jeg til leseren).

\subsection{Set cover/Mengdedekke}
Mengdedekke er et veldig generelt problem, formulert som følgende: For et grunsett $E = \{e_1,e_1,...,e_m\}$ har vi flere subset $S_1, S_2,...,S_n$ der $S_i\subseteq E$, samt en tilknyttet vekt $w_i$ for hver $S_i$. Målet er å finne den sammensetningen av subset som dekker hele grunsettet med lavest vekt. Dersom $w_i=1, \forall i \in \{1,..,n\}$ har vi et \emph{uvektet} mengdedekke problem. Formulert som et lineær program har vi altså følgende:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\limits_{i = 1}^{n} &w_ix_i &\\
\text{subject to:}&\displaystyle\sum\nolimits_{j:e \in S_j} &x_j \geq 1,  &\text{ for alle $e\in E$}\\
                 &                                                &x_i \in \{0,1\}, & i \in \{1,...,n\}
\end{array}
\end{equation*}

F.eks. er da et nodedekke et spesialtilfelle av mengdedekke. For å redusere fra en generell instans av et nodedekke til mengdedekke, la grunsettet være alle kanter. For hver node, lag et subset med alle tilkoblede kanter (slik at $n=|V|$), og la $w_i = v_i$. 

I øving 2 er det et annet eksempel der mengdedekke reduseres til \emph{uncapacitated facility problem} og motsatt.

\subsubsection{Approksimering ved avrunding}\label{{sec:approksavrunding}}
Den første taktikken for å approksimere mengdedekke er å approksimere ved avrunding. Det fungerer ved først å løse slakkingen til lineærprogrammet, finne $f = \text{max}(|\{j:e\in S_j\}|)$, og deretter runde alle $x_i \geq \frac{1}{f}$ opp til $1$, og sette resten til $0$. Dette gir en $f$-approksimering. En intuitiv tolkning av f her er maksimalt antal subset et element opptrer i. Det er tydelig at dette fremdeles er en gyldig løsning til lineærprogrammet, ettersom $\sum\nolimits_{j:e \in S_j} x_j \geq 1$ i den optimale løsningen (siden summen ikke har mer enn $f$ ledd må minst et av dem være mer enn $\frac{1}{f}$).

Dette er grunnen til at relaksering med avrunding løser nodedekke som en $2$-approksimasjon. Hver kant (hvert element) er intil nøyaktig to noder (del av to subset). Dermed har vi $f=2$.


\subsubsection{Approksimering ved dualbasert avrunding}
Dualen til (LP relakseringen av) lineærprogrammet til mengdedekket ser ut som følger:

\begin{equation*}\label{DUALsetCover}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\limits_{i = 1}^{n} &y_i &\\
\text{subject to:}&\displaystyle\sum\limits_{i : e_i \in S_j} &y_i \leq w_j,  &\text{ for alle $j\in \{1,...,n\}$}\\
                 &                                                &y_i \geq 0 &\text{ for alle $i\in \{1,...,n\}$}
\end{array}
\end{equation*}

Husk: I primalen hadde vi 1 restriksjon pr. element. Dvs. at hver $y_i$ tilsvarer en restriksjon, eller et element. I tillegg har vi i dualen en restriksjon pr. delmengde i primalen. Den intuitive moten å forstå dualen på kan være at hvert element har en pris. Denne prisen til et sett (summen av prisen til alle elemntene i settet) kan ikke overskride vekten av settet. Målet er da å maksimere prisene $y_i$.

Ideen er nå å bruke noe av konseptet bak komplimentær slakkhet, samt den optimale løsningen til dualen for å konstruere en god approksimasjon til primalen. Vi gjør det på følgende måte: Løser dualen og får $\boldsymbol{y^*_i}$. Deretter løser vi primalen ved å velge alle de $S_j$ som er slik at $\sum\nolimits_{i:e_i \in S_j}\boldsymbol{y^*_i} = w_j$, dvs der dualrestriksjonen er stram. Dette fungerer ettersom hvis det skulle ha fantes et element som ikke var dekket, ville det betydd restriksjonen ikke var stram for noen av mengdene som elementet var del av. Dermed er løsningen heller ikke optimal siden vi kunne uten problem økt prisen på elementet.

Hva får vi fra dette? Jo, det blir en ny $f$-approksimasjon, f samme som i forrige seksjon. Det er hovedsaklig to ting som er sentrale for å se dette. For det første, siden $DUAL \leq OPT$, har vi at $\sum\limits_{i=1}^{n}y_i \leq OPT$. Videre er $S_j$ kun med i løsningen dersom $\sum\nolimits_{i:e_i\in S_j}y_i = w_j$. Resten klarer du å sette sammen selv (eller se i boka...).

\subsubsection{Approksimering ved grådighet}
Den siste metoden for approksimering av mengdedekke er presentert som en grådig løsning (og her den suverent beste!). Dette er en $H_n$-approksimering ($H_n = \sum\limits_{k=1}^{n}\frac{1}{k}$, AKA. summen av de n første leddene i den harmoniske rekken), der $n=|V|$. Den er ganske enkel å forklare. Approksimasjonsgraden er litt mer å forklare. Algoritmen fungerer slik: Ved hvert steg, regn ut det settet som gir mest lavest stykkpris på udekte elementer, og legg til dette settet i løsningen. Itererer til alle elemter er dekket (maks n ganger). Litt mer formelt, la $I$ være den nåverende løsningen, og la $\hat{S_i} = S_i\setminus\{e\in I\}$. Ved hver iterasjon gjør $I = I \cup S_j : j = min_i(\frac{|S_i|}{w_i}), \forall i$. 

For bevis av approksimasjonsgraden her, se boka, side 108.
\newpage

\section{Primal-Dual Metoden}
Dette er den generelle ungarske metode/dualbasert avrunding i mengdedekke metoden, og gir ofte opphav til gode approksimeringsalgoritmer. I tillegg er approksimeringsgraden ofte lett å analysere.

Metoden er veldig generell, og går ut på å starte med en gyldig dual-løsning (ikke nødvendigvis opptimal), og så gradvis forbedre dualen samtidig som man bygger løsningen til primalen (ofte basert på komplimentær slakkhet). Analysen av løsningen går da ofte ut på at vi har komplimentær slakkhet en vei, mens en approksimert komplimentær slakkhet den andre veien. Det enkleste er nesten bare å se på eksempler, så vi kjører på.

\subsection{Mengdedekke revisited}
Se over for en beskrivelse av mengdedekke. Primal-dual metoden for mengdedekke gir opphav for løsningen approksimering ved dualbasert avrunding, kun med en liten endring. I steden for å starte med å konstruere en optimal løsning til dualen $\boldsymbol{y^*_i}$, så starter vi heller med en gyldig løsning, f.eks $y_i = 0, \forall i$. Deretter gjør vi som i analysen av gyldigheten, og ser på et vilkårlig udekt element $e_i$. Siden elementet er udekt, betyr det at $\sum\nolimits_{k:e_k\in S_j}y_k < w_j,\forall j:e_i \in S_j$, og dermed kan vi øke $y_i$ med en $\epsilon > 0$, slik at minst én dualrestriksjon blir stram. Da legger vi til $S_j$ i løsningen for alle $j : \sum\nolimits_{k:e_k\in S_j}y_k = w_j$, altså der dualrestriksjonen er stram. Slik kan vi bygge løsningen vår med maks $n = \text{antall elementer}$ iterasjoner.

Her ser vi at komplimentær slakkhet åpenbart er oppfylt den ene veien, siden vi valgte å sette $x_j = 1 \iff \sum\nolimits_{k:e_k\in S_j}y_k = w_j$. For alle andre har vi $x_k = 0$. Andre veien derimot har vi kun approksimert komplimentær slakkhet. Dette fordi at for en $y_i > 0$ så vil $\sum\nolimits_{j : y_i \in S_j}x_j \leq f$, der $f$ er som tidligere (maks antall ganger et element opptrer i forskjellige set). Dermed er det også lett og se at dette blir en $f$-approksimasjon.

\subsection{Feedback Vertex Set/Sykelkritiske Noder}
Dette problemet kan tolkes på flere måter. Personlig er den mest intuitive måten som følger: Vi har en urettet graf $G = (V,E)$, med tilhørene vekter $w_i$ til nodene. Vi ønsker å fjerne noder billigst mulig slik at $G$ blir asyklisk. Som et IP (der $\boldsymbol{C}$ betegner settet av sykler i $G$):

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\nolimits_{i \in V} w_i&x_i &\\
\text{subject to:}&\displaystyle\sum\nolimits_{i \in C} &x_i \geq 1,  &\forall C \in \boldsymbol{C}\\
                 &                                                &x_i \in \{0,1\} &\forall i \in V
\end{array}
\end{equation*}

Her er det viktig å se at $\boldsymbol{C}$ vokser eksponensielt med antall noder, så det blir mange restrikssjoner etterhvert. Tar vi relakseringen av IP formuleringen vår, og ser på dualen har vi

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\limits_{C \in \boldsymbol{C}} &y_c &\\
\text{subject to:}&\displaystyle\sum\limits_{C \in \boldsymbol{C}:i \in C} &y_c \leq w_i,  &\forall i \in V\\
                 &                                                &y_c \geq 0 &\forall C \in \boldsymbol{C}
\end{array}
\end{equation*}

Nå har vi naturlig nok eksponensielt mange dualvariabler (siden vi hadde eksponensielt mange primalrestriksjoner), men er ikke et stort problem, ettersom vi kun trenger å øke et polynisk antall av dem). Den naturlige måten å løse dette på er å forsøke nøyaktig samme strategi som i forrige seksjon, starte med gyldig dual, også øke den "nærmeste" $y_c$ slik at minst en restriksjon blir stram. Samme analyse som tidligere oppnår vi da følgende approksimasjonsgrad (her betegner $S$ løsningssettet):

\begin{align} \label{eq_01}
	&\sum\limits_{i \in S}w_i = \sum\limits_{i \in S}\sum\limits_{C:i\in C}y_c = \sum\limits_{C \in \boldsymbol{C}}|S\cap C|y_c
\end{align}

Her ser vi altså at vi får en approksimasjonsgrad på $max_{C}(|S\cap C|)$ (maks antall sykler en node i løsningen er med i). Dessverre vil samme naive "bare øk hvilken som helst dualvariabel" strategi som tidligere føre til at dette tallet kan bli veldig høyt. Men ved litt smartere valg av variabler vil vi kunne oppnå en ganske god approksimasjonsgrad.

Løsningen er å i et hvert steg kun øke dualvariabler som tilhører sykler med mindre enn $2\lceil \log_2n\rceil$ noder av grad $3$ eller mer. Det er en del analyse bak dette, men her kommer en rask oppsummering: Vi fjerner alltid noder av grad $< 2$ siden disse umulig kan være del av en sykel. Stier av noder av grad $2$ vil utslettes ved å velge en vilkårlig node i stien (dette gjøres iteratit siden naboer da vil ha grad $< 2$). Dermed kan disse stiene slåes sammen til en kant. Deretter har vi et lemma som sier at i en graf uten noder av grad $1$ finnes det alltid en sykel med maks $2\lceil \log_2n\rceil$ noder av grad $3$ eller mer. Dette sees ved å se for seg et binærsøk gjennom grafen, der vi slår sammen stier av noder med grad $2$. Her vil entall noder minst doble seg per nivå gjennom treet, og vi kan ha maksimal dybde på $\lceil \log_2n\rceil$. Dermed (om det finnes minst en sykel), vil det finnes minst en sykel som også har lengde mindre enn $2\lceil \log_2n\rceil$ (se for deg at den knyttes sammen helt i bånn av treet).

Denne strategien gir oss en $4\lceil \log_2n\rceil$-approksimasjon. For å se dette, se først at størrelsen på sykler som "legges til" (dualvariablen økes) i hvert trinn av algoritmen ikke overskrider $2\lceil \log_2n\rceil$, hvis vi kun teller noder av grad $> 2$. Teller vi bare disse nodene, så har vi $|S\cap C| \leq 2\lceil \log_2n\rceil$. Men, det kan også være stier med noder av grad $2$ i mellom hver av nodene. Uansett, fra disse stiene er maksimalt en node med (se forrige avsnitt). Dermed kan har vi $|S\cap C| \leq 4\lceil \log_2n\rceil$, og som tidligere argumentert for gir dette en øvre grense på approksimasjonsgraden.

Dette er en viktig lekse i primal-dual algoritmen. Ofte ønsker vi å øke den "minste" (på et eller annet vis) dual-variabelen.

\subsection{Korteste vei}
Mer av det samme som i forrige seksjon, det vanskeligste her er oppsettet. Hopper over det formelle, kan evt. se i boka. Her har vi primalvariabler som kanter som skal skrus av/på, mens dualvariablene er s-t kutt, dvs. $S = \{v \in V:s \in S, v \notin S\}$. Er egentlig ingen ny lekse her, men igjen øker vi alltid den $y_s$ som tilhører den "minste" $S \in \boldsymbol{S}$. Minste her betyr det subsettet av noder vi kan nå med løsningen vi har bygget til ved den nåværende iterasjonen, og øker til en av dualrestriksjonene blir stramme. Da tar vi med den kanten i løsningen. Tilslutt, når vi når $t$ så stripper vi vekk alt som ikke er del av s-t veien og returnerer løsningen $P$.

Resultatet her blir da en velkjent algoritme, Dijkstra's algoritme, som finner den optimale løsningen. Dette sees ved standard primal-dual analyse, som gir oss at dette er en $max(|P\cap \delta (S)|)$-approksimasjon, der $\delta (S) = \{e = (u,v): u \in S, v \notin S \}$. Deretter kan man se at $|P\cap \delta (S)| = 1, \forall y_c \neq 0$, siden ellers ville vi på et punkt lagt til en sykel i løsningen vår. Dermed får vi en optimal løsning (det er jo velkjent at Dijkstra's returnerer den optimale løsningen, så dette burde vel ikke være noen bombe).

\subsection{Minimum knapsack problem}
Denne seksjoner viser et annet fenomen som kan være et hinder for approksimeringsalgoritmer. Problemet er som et slags motsatt knapsack-problem, der man ønsker å bære lavest mulig vekt, men der verdien av tingene er høyere enn en satt terskel. En naturlig formulering av IP-et gir:

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\limits_{i \in I} s_i&x_i &\\
\text{subject to:}&\displaystyle\sum\limits_{i \in I} v_i&x_i \geq D\\
                 &                                                &x_i \in \{0,1\} &\forall i \in I
\end{array}
\end{equation*}

Her er $s_i$ og $v_i$ hhv. vekten og verdien til objektet $i \in I$. Dette oppsettet gir opphav til et arbitrært dårlig "Integrality gap", eller heltallsavvik. Heltallsavvik er betegnelsen på den øvre grensen for et tall $\alpha$ slik at $\frac{OPT}{PRIM} \geq \alpha$ (for et minimeringsproblem). I primal-dual analysen er det helt klart nødvendig at dette tallet ikke er for høyt, ettersom det gjør at løsningen på relakseringen forteller oss lite om den faktiske løsningen. 

I vårt nåværende oppsett kan man gi problemet $I = \{1,2\}$, hvor $v_1 = D-1, v_2 = D, s_1 = 0, s_2 = 1$. Her er det lett og se at heltallsløsningen er å kun ta med objekt $2$, og gir oss summen $1$, mens LP-relakseringen vil ta med objekt $1$, og fylle opp siste lille biten med en fraksjon av objekt $2$. Dette gir summen $\frac{1}{D}$, og gir oss heltallsavvik $\frac{OPT}{PRIM} = D$.

Det er en annen lur måte å sette opp problemet på som unngår det skrekkelige heltallsavviket ved å bytte ut verdi-begrepet med "avstand" fra å fylle opp til kravet D, og gir opphav til en $2$-approksimering. Se boka (s. 127)/forelesning 5 for detaljer.
\newpage
\newpage
%% EXAMPLE ALGORITHM
\begin{lstlisting}[language=python]
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Layer, Activation
import time
import tensorflow as tf

f = open("results.csv", "w")


INPUT_SIZE = 10
OUTPUT_SIZE = INPUT_SIZE
nb_class = 3

batch_size = 128
nb_epoch = 40

np.random.seed(123)

X_train = np.random.rand(INPUT_SIZE, nb_class)
Y_train = np.random.rand(OUTPUT_SIZE, nb_class)

X_test = np.random.rand(INPUT_SIZE)
Y_test = np.random.rand(OUTPUT_SIZE)

for i in range(1,51):

    start_time = time.time()

    model = Sequential()
    model.add(Dense(INPUT_SIZE, input_shape=(nb_class,)))
    model.add(Activation('linear'))
    model.add(Dense(OUTPUT_SIZE))
    model.add(Activation('linear'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

    final_time = time.time()
    diff_time = final_time - start_time

    f.write(str(i)+","+str(diff_time)+","+"\n")

f.close() 
\end{lstlisting}

\end{document}
