%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{
    \documentclass[twoside,11pt]{article}
    %%%%% PACKAGES %%%%%%
    \usepackage{pgm2016}
    \usepackage{amsmath}
    \usepackage{algorithm}
    \usepackage[noend]{algpseudocode}
    \usepackage{subcaption}
    \usepackage[english]{babel}	
    \usepackage{paralist}	
    \usepackage[lowtilde]{url}
    \usepackage{fixltx2e}
    \usepackage{listings}
    \usepackage{color}
    \usepackage{hyperref}
    
    \usepackage{auto-pst-pdf}
    \usepackage{pst-all}
    \usepackage{pstricks-add}
    
    %%%%% MACROS %%%%%%
    \algrenewcommand\Return{\State \algorithmicreturn{} }
    \algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}
    \renewcommand{\thesubfigure}{\roman{subfigure}}
    \definecolor{codegreen}{rgb}{0,0.6,0}
    \definecolor{codegray}{rgb}{0.5,0.5,0.5}
    \definecolor{codepurple}{rgb}{0.58,0,0.82}
    \definecolor{backcolour}{rgb}{0.95,0.95,0.92}
    \lstdefinestyle{mystyle}{
       backgroundcolor=\color{backcolour},  
       commentstyle=\color{codegreen},
       keywordstyle=\color{magenta},
       numberstyle=\tiny\color{codegray},
       stringstyle=\color{codepurple},
       basicstyle=\footnotesize,
       breakatwhitespace=false,        
       breaklines=true,                
       captionpos=b,                    
       keepspaces=true,                
       numbers=left,                    
       numbersep=5pt,                  
       showspaces=false,                
       showstringspaces=false,
       showtabs=false,                  
       tabsize=2
    }
    \lstset{style=mystyle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }

%%%%%%%%%%%%%%%%%%%%%%%% CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% {
\newcommand\course{CS 330-001}
\newcommand\courseName{Introduction to Operating Systems}
\newcommand\semester{Winter 2020}
\newcommand\assignmentNumber{1}                             % <-- ASSIGNMENT #
\newcommand\studentName{Your Name}                  % <-- YOUR NAME
\newcommand\studentEmail{email@uregina.ca}          % <-- YOUR NAME
\newcommand\studentNumber{200XXYYZZ}                % <-- STUDENT ID #
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{


    \firstpageno{1}
    
    \begin{document}
    
    \title{AlgKons(entrert) Kompendium}

    \maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }
\tableofcontents
\newpage
\section{Matching}
\section{Lineær Programmering}
\subsection{Komplimentær slakkhet}
\section{Den ungarske metoden}
\section{Approksimasjon}
\label{sec:approx}


Approksimasjonsalgoritmer er algoritmer for å løse optimaliseringsproblemer. Konseptet som introduseres er litt løst basert på følgende utsagn: For å løse optimaliseringsproblemer trenger man 1) Optimal løsning, 2) I rask tid, 3) for alle problemer. Velg 2. Approksimeringsalgoritmer slacker på kravet om optimal løsning, ved å heller gi et svar som er så nært som mulig optimalt, men ikke nødvendigvis optimalt.

For å få noe av verdi må vi kunne si noe om hvor nære approksimasjonen er, eller approksimasjonsgraden. La APX og OPT være hhv. approksimasjonen og den optimale løsningen. Da sier vi at algoritmen er en $\alpha$-approksimasjon dersom.

\begin{align} \label{eq_01}
	&APX \geq \frac{1}{\alpha}OPT \text{ For maksimeringsproblemer}\\ 		
	&\frac{1}{\alpha}APX \leq OPT \text{ For minimeringsproblemer}
\end{align}

Et viktig konsept som går igjen er PTAS vs FPTAS. Definisjonen på en PTAS er en familie med approksimeringsalgoritmer $\{A_\epsilon\}_{\epsilon>0}$, der alle $A \in \{A_\epsilon\}_{\epsilon>0}$ er ($1 + \epsilon$)-approksimasjoner, med polynomisk kjøretid i n. En FPTAS er en PTAS der kjøretiden også er polynomisk i $\frac{1}{\epsilon}$. For å illustrere forskjellen oppfyller $O(n^{\frac{1}{\epsilon}})$ kun kravet til PTAS, mens $O(n^c + (\frac{1}{\epsilon})^d)$ oppfyller kravet til FPTAS også.

\subsection{Maximum Independent set/Største Uavhengige Set}
For en graf $G = (V,E)$, kalles en $I\subseteq V$ independent/Uavhengig dersom ingen to noder i $I$ er koblet sammen.

Å finne det største kan skrives som et lineærprogram på følgende måte

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\nolimits_{v\in V} &x_v &\\
\text{subject to:}&x_u + &x_c \geq 1,  &\text{ for alle kanter $(u,v)\in E$}\\
                 &                                                &x_v \in \{0,1\}, &\text{ for alle noder $v\in V$}
\end{array}
\end{equation*}

Dette er hovedsaklig tatt med som et eksempel på problemer som ikke kan approksimeres (Ikke 100\% sikker på dette, men tror det er et teorem om at det ikke finnes noen PTAS for største uavhengige sett, med mindre $P = NP$). Det samme gjelder for Maksimum Clique problemet (Hvordan disse problemene er relatert overlater jeg til leseren).

\subsection{Set cover/Mengdedekke}
Mengdedekke er et veldig generelt problem, formulert som følgende: For et grunsett $E = \{e_1,e_1,...,e_m\}$ har vi flere subset $S_1, S_2,...,S_n$ der $S_i\subseteq E$, samt en tilknyttet vekt $w_i$ for hver $S_i$. Målet er å finne den sammensetningen av subset som dekker hele grunsettet med lavest vekt. Dersom $w_i=1, \forall i \in \{1,..,n\}$ har vi et \emph{uvektet} mengdedekke problem. Formulert som et lineær program har vi altså følgende:

\begin{equation*}\label{maxindependentset}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\limits_{i = 1}^{n} &w_ix_i &\\
\text{subject to:}&\displaystyle\sum\nolimits_{j:e \in S_j} &x_j \geq 1,  &\text{ for alle $e\in E$}\\
                 &                                                &x_i \in \{0,1\}, & i \in \{1,...,n\}
\end{array}
\end{equation*}

F.eks. er da et nodedekke et spesialtilfelle av mengdedekke. For å redusere fra en generell instans av et nodedekke til mengdedekke, la grunsettet være alle kanter. For hver node, lag et subset med alle tilkoblede kanter (slik at $n=|V|$), og la $w_i = v_i$. 

I øving 2 er det et annet eksempel der mengdedekke reduseres til \emph{uncapacitated facility problem} og motsatt.

\subsubsection{Approksimering ved avrunding}\label{{sec:approksavrunding}}
Den første taktikken for å approksimere mengdedekke er å approksimere ved avrunding. Det fungerer ved først å løse slakkingen til lineærprogrammet, finne $f = \text{max}(|\{j:e\in S_j\}|)$, og deretter runde alle $x_i \geq \frac{1}{f}$ opp til $1$, og sette resten til $0$. Dette gir en $f$-approksimering. En intuitiv tolkning av f her er maksimalt antal subset et element opptrer i. Det er tydelig at dette fremdeles er en gyldig løsning til lineærprogrammet, ettersom $\sum\nolimits_{j:e \in S_j} x_j \geq 1$ i den optimale løsningen (siden summen ikke har mer enn $f$ ledd må minst et av dem være mer enn $\frac{1}{f}$).

Dette er grunnen til at relaksering med avrunding løser nodedekke som en $2$-approksimasjon. Hver kant (hvert element) er intil nøyaktig to noder (del av to subset). Dermed har vi $f=2$.


\subsubsection{Approksimering ved dualbasert avrunding}
Dualen til (LP relakseringen av) lineærprogrammet til mengdedekket ser ut som følger:

\begin{equation*}\label{DUALsetCover}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\limits_{i = 1}^{n} &y_i &\\
\text{subject to:}&\displaystyle\sum\limits_{i : e_i \in S_j} &y_i \leq w_j,  &\text{ for alle $j\in \{1,...,n\}$}\\
                 &                                                &y_i \geq 0 &\text{ for alle $i\in \{1,...,n\}$}
\end{array}
\end{equation*}

Husk: I primalen hadde vi 1 restriksjon pr. element. Dvs. at hver $y_i$ tilsvarer en restriksjon, eller et element. I tillegg har vi i dualen en restriksjon pr. delmengde i primalen. Den intuitive moten å forstå dualen på kan være at hvert element har en pris. Denne prisen til et sett (summen av prisen til alle elemntene i settet) kan ikke overskride vekten av settet. Målet er da å maksimere prisene $y_i$.

Ideen er nå å bruke noe av konseptet bak komplimentær slakkhet, samt den optimale løsningen til dualen for å konstruere en god approksimasjon til primalen. Vi gjør det på følgende måte: Løser dualen og får $\boldsymbol{y^*_i}$. Deretter løser vi primalen ved å velge alle de $S_j$ som er slik at $\sum\nolimits_{i:e_i \in S_j}\boldsymbol{y^*_i} = w_j$, dvs der dualrestriksjonen er stram. Dette fungerer ettersom hvis det skulle ha fantes et element som ikke var dekket, ville det betydd restriksjonen ikke var stram for noen av mengdene som elementet var del av. Dermed er løsningen heller ikke optimal siden vi kunne uten problem økt prisen på elementet.

Hva får vi fra dette? Jo, det blir en ny $f$-approksimasjon, f samme som i forrige seksjon. Det er hovedsaklig to ting som er sentrale for å se dette. For det første, siden $DUAL \leq OPT$, har vi at $\sum\limits_{i=1}^{n}y_i \leq OPT$. Videre er $S_j$ kun med i løsningen dersom $\sum\nolimits_{i:e_i\in S_j}y_i = w_j$. Resten klarer du å sette sammen selv (eller se i boka...).

\subsubsection{Approksimering ved grådighet}
Den siste metoden for approksimering av mengdedekke er presentert som en grådig løsning (og her den suverent beste!). Dette er en $H_n$-approksimering ($H_n = \sum\limits_{k=1}^{n}\frac{1}{k}$, AKA. summen av de n første leddene i den harmoniske rekken), der $n=|V|$. Den er ganske enkel å forklare. Approksimasjonsgraden er litt mer å forklare. Algoritmen fungerer slik: Ved hvert steg, regn ut det settet som gir mest lavest stykkpris på udekte elementer, og legg til dette settet i løsningen. Itererer til alle elemter er dekket (maks n ganger). Litt mer formelt, la $I$ være den nåverende løsningen, og la $\hat{S_i} = S_i\setminus\{e\in I\}$. Ved hver iterasjon gjør $I = I \cup S_j : j = min_i(\frac{|S_i|}{w_i}), \forall i$. 

For bevis av approksimasjonsgraden her, se boka, side 108.
\newpage

\section{Primal-Dual Metoden}
Dette er den generelle ungarske metode/dualbasert avrunding i mengdedekke metoden, og gir ofte opphav til gode approksimeringsalgoritmer. I tillegg er approksimeringsgraden ofte lett å analysere.

Metoden er veldig generell, og går ut på å starte med en gyldig dual-løsning (ikke nødvendigvis opptimal), og så gradvis forbedre dualen samtidig som man bygger løsningen til primalen (ofte basert på komplimentær slakkhet). Analysen av løsningen går da ofte ut på at vi har komplimentær slakkhet en vei, mens en approksimert komplimentær slakkhet den andre veien. Det enkleste er nesten bare å se på eksempler, så vi kjører på.

\subsection{Mengdedekke revisited}
Se over for en beskrivelse av mengdedekke. Primal-dual metoden for mengdedekke gir opphav for løsningen approksimering ved dualbasert avrunding, kun med en liten endring. I steden for å starte med å konstruere en optimal løsning til dualen $\boldsymbol{y^*_i}$, så starter vi heller med en gyldig løsning, f.eks $y_i = 0, \forall i$. Deretter gjør vi som i analysen av gyldigheten, og ser på et vilkårlig udekt element $e_i$. Siden elementet er udekt, betyr det at $\sum\nolimits_{k:e_k\in S_j}y_k < w_j,\forall j:e_i \in S_j$, og dermed kan vi øke $y_i$ med en $\epsilon > 0$, slik at minst én dualrestriksjon blir stram. Da legger vi til $S_j$ i løsningen for alle $j : \sum\nolimits_{k:e_k\in S_j}y_k = w_j$, altså der dualrestriksjonen er stram. Slik kan vi bygge løsningen vår med maks $n = \text{antall elementer}$ iterasjoner.

Her ser vi at komplimentær slakkhet åpenbart er oppfylt den ene veien, siden vi valgte å sette $x_j = 1 \iff \sum\nolimits_{k:e_k\in S_j}y_k = w_j$. For alle andre har vi $x_k = 0$. Andre veien derimot har vi kun approksimert komplimentær slakkhet. Dette fordi at for en $y_i > 0$ så vil $\sum\nolimits_{j : y_i \in S_j}x_j \leq f$, der $f$ er som tidligere (maks antall ganger et element opptrer i forskjellige set). Dermed er det også lett og se at dette blir en $f$-approksimasjon.

\subsection{Feedback Vertex Set/Sykelkritiske Noder}
Dette problemet kan tolkes på flere måter. Personlig er den mest intuitive måten som følger: Vi har en urettet graf $G = (V,E)$, med tilhørene vekter $w_i$ til nodene. Vi ønsker å fjerne noder billigst mulig slik at $G$ blir asyklisk. Som et IP (der $\boldsymbol{C}$ betegner settet av sykler i $G$):

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\nolimits_{i \in V} w_i&x_i &\\
\text{subject to:}&\displaystyle\sum\nolimits_{i \in C} &x_i \geq 1,  &\forall C \in \boldsymbol{C}\\
                 &                                                &x_i \in \{0,1\} &\forall i \in V
\end{array}
\end{equation*}

Her er det viktig å se at $\boldsymbol{C}$ vokser eksponensielt med antall noder, så det blir mange restrikssjoner etterhvert. Tar vi relakseringen av IP formuleringen vår, og ser på dualen har vi

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Maximize: }  & \displaystyle\sum\limits_{C \in \boldsymbol{C}} &y_c &\\
\text{subject to:}&\displaystyle\sum\limits_{C \in \boldsymbol{C}:i \in C} &y_c \leq w_i,  &\forall i \in V\\
                 &                                                &y_c \geq 0 &\forall C \in \boldsymbol{C}
\end{array}
\end{equation*}

Nå har vi naturlig nok eksponensielt mange dualvariabler (siden vi hadde eksponensielt mange primalrestriksjoner), men er ikke et stort problem, ettersom vi kun trenger å øke et polynisk antall av dem). Den naturlige måten å løse dette på er å forsøke nøyaktig samme strategi som i forrige seksjon, starte med gyldig dual, også øke den "nærmeste" $y_c$ slik at minst en restriksjon blir stram. Samme analyse som tidligere oppnår vi da følgende approksimasjonsgrad (her betegner $S$ løsningssettet):

\begin{align} \label{eq_01}
	&\sum\limits_{i \in S}w_i = \sum\limits_{i \in S}\sum\limits_{C:i\in C}y_c = \sum\limits_{C \in \boldsymbol{C}}|S\cap C|y_c
\end{align}

Her ser vi altså at vi får en approksimasjonsgrad på $max_{C}(|S\cap C|)$ (maks antall sykler en node i løsningen er med i). Dessverre vil samme naive "bare øk hvilken som helst dualvariabel" strategi som tidligere føre til at dette tallet kan bli veldig høyt. Men ved litt smartere valg av variabler vil vi kunne oppnå en ganske god approksimasjonsgrad.

Løsningen er å i et hvert steg kun øke dualvariabler som tilhører sykler med mindre enn $2\lceil \log_2n\rceil$ noder av grad $3$ eller mer. Det er en del analyse bak dette, men her kommer en rask oppsummering: Vi fjerner alltid noder av grad $< 2$ siden disse umulig kan være del av en sykel. Stier av noder av grad $2$ vil utslettes ved å velge en vilkårlig node i stien (dette gjøres iteratit siden naboer da vil ha grad $< 2$). Dermed kan disse stiene slåes sammen til en kant. Deretter har vi et lemma som sier at i en graf uten noder av grad $1$ finnes det alltid en sykel med maks $2\lceil \log_2n\rceil$ noder av grad $3$ eller mer. Dette sees ved å se for seg et binærsøk gjennom grafen, der vi slår sammen stier av noder med grad $2$. Her vil entall noder minst doble seg per nivå gjennom treet, og vi kan ha maksimal dybde på $\lceil \log_2n\rceil$. Dermed (om det finnes minst en sykel), vil det finnes minst en sykel som også har lengde mindre enn $2\lceil \log_2n\rceil$ (se for deg at den knyttes sammen helt i bånn av treet).

Denne strategien gir oss en $4\lceil \log_2n\rceil$-approksimasjon. For å se dette, se først at størrelsen på sykler som "legges til" (dualvariablen økes) i hvert trinn av algoritmen ikke overskrider $2\lceil \log_2n\rceil$, hvis vi kun teller noder av grad $> 2$. Teller vi bare disse nodene, så har vi $|S\cap C| \leq 2\lceil \log_2n\rceil$. Men, det kan også være stier med noder av grad $2$ i mellom hver av nodene. Uansett, fra disse stiene er maksimalt en node med (se forrige avsnitt). Dermed kan har vi $|S\cap C| \leq 4\lceil \log_2n\rceil$, og som tidligere argumentert for gir dette en øvre grense på approksimasjonsgraden.

Dette er en viktig lekse i primal-dual algoritmen. Ofte ønsker vi å øke den "minste" (på et eller annet vis) dual-variabelen.

\subsection{Korteste vei}
Mer av det samme som i forrige seksjon, det vanskeligste her er oppsettet. Hopper over det formelle, kan evt. se i boka. Her har vi primalvariabler som kanter som skal skrus av/på, mens dualvariablene er s-t kutt, dvs. $S = \{v \in V:s \in S, v \notin S\}$. Er egentlig ingen ny lekse her, men igjen øker vi alltid den $y_s$ som tilhører den "minste" $S \in \boldsymbol{S}$. Minste her betyr det subsettet av noder vi kan nå med løsningen vi har bygget til ved den nåværende iterasjonen, og øker til en av dualrestriksjonene blir stramme. Da tar vi med den kanten i løsningen. Tilslutt, når vi når $t$ så stripper vi vekk alt som ikke er del av s-t veien og returnerer løsningen $P$.

Resultatet her blir da en velkjent algoritme, Dijkstra's algoritme, som finner den optimale løsningen. Dette sees ved standard primal-dual analyse, som gir oss at dette er en $max(|P\cap \delta (S)|)$-approksimasjon, der $\delta (S) = \{e = (u,v): u \in S, v \notin S \}$. Deretter kan man se at $|P\cap \delta (S)| = 1, \forall y_c \neq 0$, siden ellers ville vi på et punkt lagt til en sykel i løsningen vår. Dermed får vi en optimal løsning (det er jo velkjent at Dijkstra's returnerer den optimale løsningen, så dette burde vel ikke være noen bombe).

\subsection{Minimum knapsack problem}
Denne seksjoner viser et annet fenomen som kan være et hinder for approksimeringsalgoritmer. Problemet er som et slags motsatt knapsack-problem, der man ønsker å bære lavest mulig vekt, men der verdien av tingene er høyere enn en satt terskel. En naturlig formulering av IP-et gir:

\begin{equation*}\label{FeedbackVertices}
\begin{array}{ll@{}ll}
\text{Minimize: }  & \displaystyle\sum\limits_{i \in I} s_i&x_i &\\
\text{subject to:}&\displaystyle\sum\limits_{i \in I} v_i&x_i \geq D\\
                 &                                                &x_i \in \{0,1\} &\forall i \in I
\end{array}
\end{equation*}

Her er $s_i$ og $v_i$ hhv. vekten og verdien til objektet $i \in I$. Dette oppsettet gir opphav til et arbitrært dårlig "Integrality gap", eller heltallsavvik. Heltallsavvik er betegnelsen på den øvre grensen for et tall $\alpha$ slik at $\frac{OPT}{PRIM} \geq \alpha$ (for et minimeringsproblem). I primal-dual analysen er det helt klart nødvendig at dette tallet ikke er for høyt, ettersom det gjør at løsningen på relakseringen forteller oss lite om den faktiske løsningen. 

I vårt nåværende oppsett kan man gi problemet $I = \{1,2\}$, hvor $v_1 = D-1, v_2 = D, s_1 = 0, s_2 = 1$. Her er det lett og se at heltallsløsningen er å kun ta med objekt $2$, og gir oss summen $1$, mens LP-relakseringen vil ta med objekt $1$, og fylle opp siste lille biten med en fraksjon av objekt $2$. Dette gir summen $\frac{1}{D}$, og gir oss heltallsavvik $\frac{OPT}{PRIM} = D$.

Det er en annen lur måte å sette opp problemet på som unngår det skrekkelige heltallsavviket ved å bytte ut verdi-begrepet med "avstand" fra å fylle opp til kravet D, og gir opphav til en $2$-approksimering. Se boka (s. 127)/forelesning 5 for detaljer.
\newpage
\newpage
%% EXAMPLE ALGORITHM
\begin{lstlisting}[language=python]
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Layer, Activation
import time
import tensorflow as tf

f = open("results.csv", "w")


INPUT_SIZE = 10
OUTPUT_SIZE = INPUT_SIZE
nb_class = 3

batch_size = 128
nb_epoch = 40

np.random.seed(123)

X_train = np.random.rand(INPUT_SIZE, nb_class)
Y_train = np.random.rand(OUTPUT_SIZE, nb_class)

X_test = np.random.rand(INPUT_SIZE)
Y_test = np.random.rand(OUTPUT_SIZE)

for i in range(1,51):

    start_time = time.time()

    model = Sequential()
    model.add(Dense(INPUT_SIZE, input_shape=(nb_class,)))
    model.add(Activation('linear'))
    model.add(Dense(OUTPUT_SIZE))
    model.add(Activation('linear'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

    final_time = time.time()
    diff_time = final_time - start_time

    f.write(str(i)+","+str(diff_time)+","+"\n")

f.close() 
\end{lstlisting}

\end{document}
